{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RankLib: Learning-to-Rank Library\n",
    "\n",
    "RankLib is a Java library used for learning-to-rank algorithms. It provides various ranking algorithms for training and evaluation.\n",
    "\n",
    "### Training and Testing Models\n",
    "\n",
    "The following code snippets train and test different learning-to-rank models with specific feature sets and evaluation metrics:\n",
    "\n",
    "1. `linguistic_sentiment` model\n",
    "2. `sbert` model\n",
    "3. `sentiment_sarcasm` model\n",
    "\n",
    "```bash\n",
    "!java -jar ../ranking-model/RankLib-2.18.jar -train <train_data> -test <test_data> -tvs .2 -ranker 8 -metric2t NDCG@10 -metric2T ERR@10 -save <model_path>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: java -jar RankLib.jar <Params>\n",
      "Params:\n",
      "  [+] Training (+ tuning and evaluation)\n",
      "\t-train <file>\t\tTraining data\n",
      "\t-ranker <type>\t\tSpecify which ranking algorithm to use\n",
      "\t\t\t\t0: MART (gradient boosted regression tree)\n",
      "\t\t\t\t1: RankNet\n",
      "\t\t\t\t2: RankBoost\n",
      "\t\t\t\t3: AdaRank\n",
      "\t\t\t\t4: Coordinate Ascent\n",
      "\t\t\t\t6: LambdaMART\n",
      "\t\t\t\t7: ListNet\n",
      "\t\t\t\t8: Random Forests\n",
      "\t\t\t\t9: Linear regression (L2 regularization)\n",
      "\t[ -feature <file> ]\tFeature description file: list features to be considered by the learner, each on a separate line\n",
      "\t\t\t\tIf not specified, all features will be used.\n",
      "\t[ -metric2t <metric> ]\tMetric to optimize on the training data.  Supported: MAP, NDCG@k, DCG@k, P@k, RR@k, ERR@k (default=ERR@10)\n",
      "\t[ -gmax <label> ]\tHighest judged relevance label. It affects the calculation of ERR (default=4, i.e. 5-point scale {0,1,2,3,4})\n",
      "\t[ -qrel <file> ]\tTREC-style relevance judgment file. It only affects MAP and NDCG (default=unspecified)\n",
      "\t[ -silent ]\t\tDo not print progress messages (which are printed by default)\n",
      "\t[ -missingZero ]\tSubstitute zero for missing feature values rather than throwing an exception.\n",
      "\n",
      "\t[ -validate <file> ]\tSpecify if you want to tune your system on the validation data (default=unspecified)\n",
      "\t\t\t\tIf specified, the final model will be the one that performs best on the validation data\n",
      "\t[ -tvs <x \\in [0..1]> ]\tIf you don't have separate validation data, use this to set train-validation split to be (x)(1.0-x)\n",
      "\t[ -save <model> ]\tSave the model learned (default=not-save)\n",
      "\n",
      "\t[ -test <file> ]\tSpecify if you want to evaluate the trained model on this data (default=unspecified)\n",
      "\t[ -tts <x \\in [0..1]> ]\tSet train-test split to be (x)(1.0-x). -tts will override -tvs\n",
      "\t[ -metric2T <metric> ]\tMetric to evaluate on the test data (default to the same as specified for -metric2t)\n",
      "\n",
      "\t[ -norm <method>]\tNormalize all feature vectors (default=no-normalization). Method can be:\n",
      "\t\t\t\tsum: normalize each feature by the sum of all its values\n",
      "\t\t\t\tzscore: normalize each feature by its mean/standard deviation\n",
      "\t\t\t\tlinear: normalize each feature by its min/max values\n",
      "\n",
      "\t[ -kcv <k> ]\t\tSpecify if you want to perform k-fold cross validation using the specified training data (default=NoCV)\n",
      "\t\t\t\t-tvs can be used to further reserve a portion of the training data in each fold for validation\n",
      "\t[ -kcvmd <dir> ]\tDirectory for models trained via cross-validation (default=not-save)\n",
      "\t[ -kcvmn <model> ]\tName for model learned in each fold. It will be prefix-ed with the fold-number (default=empty)\n",
      "\n",
      "    [-] RankNet-specific parameters\n",
      "\t[ -epoch <T> ]\t\tThe number of epochs to train (default=100)\n",
      "\t[ -layer <layer> ]\tThe number of hidden layers (default=1)\n",
      "\t[ -node <node> ]\tThe number of hidden nodes per layer (default=10)\n",
      "\t[ -lr <rate> ]\t\tLearning rate (default=0.00005)\n",
      "\n",
      "    [-] RankBoost-specific parameters\n",
      "\t[ -round <T> ]\t\tThe number of rounds to train (default=300)\n",
      "\t[ -tc <k> ]\t\tNumber of threshold candidates to search. -1 to use all feature values (default=10)\n",
      "\n",
      "    [-] AdaRank-specific parameters\n",
      "\t[ -round <T> ]\t\tThe number of rounds to train (default=500)\n",
      "\t[ -noeq ]\t\tTrain without enqueuing too-strong features (default=unspecified)\n",
      "\t[ -tolerance <t> ]\tTolerance between two consecutive rounds of learning (default=0.002)\n",
      "\t[ -max <times> ]\tThe maximum number of times a feature can be consecutively selected without changing performance (default=5)\n",
      "\n",
      "    [-] Coordinate Ascent-specific parameters\n",
      "\t[ -r <k> ]\t\tThe number of random restarts (default=5)\n",
      "\t[ -i <iteration> ]\tThe number of iterations to search in each dimension (default=25)\n",
      "\t[ -tolerance <t> ]\tPerformance tolerance between two solutions (default=0.001)\n",
      "\t[ -reg <slack> ]\tRegularization parameter (default=no-regularization)\n",
      "\n",
      "    [-] {MART, LambdaMART}-specific parameters\n",
      "\t[ -tree <t> ]\t\tNumber of trees (default=1000)\n",
      "\t[ -leaf <l> ]\t\tNumber of leaves for each tree (default=10)\n",
      "\t[ -shrinkage <factor> ]\tShrinkage, or learning rate (default=0.1)\n",
      "\t[ -tc <k> ]\t\tNumber of threshold candidates for tree spliting. -1 to use all feature values (default=256)\n",
      "\t[ -mls <n> ]\t\tMin leaf support -- minimum % of docs each leaf has to contain (default=1)\n",
      "\t[ -estop <e> ]\t\tStop early when no improvement is observed on validaton data in e consecutive rounds (default=100)\n",
      "\n",
      "    [-] ListNet-specific parameters\n",
      "\t[ -epoch <T> ]\t\tThe number of epochs to train (default=1500)\n",
      "\t[ -lr <rate> ]\t\tLearning rate (default=0.00001)\n",
      "\n",
      "    [-] Random Forests-specific parameters\n",
      "\t[ -bag <r> ]\t\tNumber of bags (default=300)\n",
      "\t[ -srate <r> ]\t\tSub-sampling rate (default=1.0)\n",
      "\t[ -frate <r> ]\t\tFeature sampling rate (default=0.3)\n",
      "\t[ -rtype <type> ]\tRanker to bag (default=0, i.e. MART)\n",
      "\t[ -tree <t> ]\t\tNumber of trees in each bag (default=1)\n",
      "\t[ -leaf <l> ]\t\tNumber of leaves for each tree (default=100)\n",
      "\t[ -shrinkage <factor> ]\tShrinkage, or learning rate (default=0.1)\n",
      "\t[ -tc <k> ]\t\tNumber of threshold candidates for tree spliting. -1 to use all feature values (default=256)\n",
      "\t[ -mls <n> ]\t\tMin leaf support -- minimum % of docs each leaf has to contain (default=1)\n",
      "\n",
      "    [-] Linear Regression-specific parameters\n",
      "\t[ -L2 <reg> ]\t\tL2 regularization parameter (default=1.0E-10)\n",
      "\n",
      "  [+] Testing previously saved models\n",
      "\t-load <model>\t\tThe model to load\n",
      "\t\t\t\tMultiple -load can be used to specify models from multiple folds (in increasing order),\n",
      "\t\t\t\t  in which case the test/rank data will be partitioned accordingly.\n",
      "\t-test <file>\t\tTest data to evaluate the model(s) (specify either this or -rank but not both)\n",
      "\t-rank <file>\t\tRank the samples in the specified file (specify either this or -test but not both)\n",
      "\t[ -metric2T <metric> ]\tMetric to evaluate on the test data (default=ERR@10)\n",
      "\t[ -gmax <label> ]\tHighest judged relevance label. It affects the calculation of ERR (default=4, i.e. 5-point scale {0,1,2,3,4})\n",
      "\t[ -score <file>]\tStore ranker's score for each object being ranked (has to be used with -rank)\n",
      "\t[ -qrel <file> ]\tTREC-style relevance judgment file. It only affects MAP and NDCG (default=unspecified)\n",
      "\t[ -idv <file> ]\t\tSave model performance (in test metric) on individual ranked lists (has to be used with -test)\n",
      "\t[ -norm ]\t\tNormalize feature vectors (similar to -norm for training/tuning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 2020 Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Training data:\t../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2020.svmrank\n",
      "Test data:\t../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2021.svmrank\n",
      "Train-Validation split: 0.2\n",
      "Feature vector representation: Dense.\n",
      "Ranking method:\tLambdaMART\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@10\n",
      "Test metric:\tERR@10\n",
      "Highest relevance label (to compute ERR): 4\n",
      "Feature normalization: No\n",
      "Model file: ../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2020_LambdaMart.txt\n",
      "\n",
      "[+] LambdaMART's Parameters:\n",
      "No. of trees: 1000\n",
      "No. of leaves: 10\n",
      "No. of threshold candidates: 256\n",
      "Min leaf support: 1\n",
      "Learning rate: 0.1\n",
      "Stop early: 100 rounds without performance gain on validation data\n",
      "\n",
      "Reading feature file [../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2020.svmrank]... [Done.]            \n",
      "(49 ranked lists, 1806 entries read)\n",
      "Reading feature file [../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2021.svmrank]... [Done.]            \n",
      "(50 ranked lists, 3442 entries read)\n",
      "Initializing... [Done]\n",
      "---------------------------------\n",
      "Training starts...\n",
      "---------------------------------\n",
      "#iter   | NDCG@10-T | NDCG@10-V | \n",
      "---------------------------------\n",
      "1       | 0.6484    | 0.5757    | \n",
      "2       | 0.706     | 0.5798    | \n",
      "3       | 0.7198    | 0.5813    | \n",
      "4       | 0.7198    | 0.5872    | \n",
      "5       | 0.7203    | 0.5876    | \n",
      "6       | 0.7472    | 0.5819    | \n",
      "7       | 0.7416    | 0.5821    | \n",
      "8       | 0.7482    | 0.5846    | \n",
      "9       | 0.7418    | 0.583     | \n",
      "10      | 0.7504    | 0.5909    | \n",
      "11      | 0.7587    | 0.595     | \n",
      "12      | 0.7555    | 0.5971    | \n",
      "13      | 0.7567    | 0.5952    | \n",
      "14      | 0.7603    | 0.593     | \n",
      "15      | 0.7625    | 0.5906    | \n",
      "16      | 0.7661    | 0.5948    | \n",
      "17      | 0.773     | 0.5907    | \n",
      "18      | 0.774     | 0.5872    | \n",
      "19      | 0.7786    | 0.5936    | \n",
      "20      | 0.7895    | 0.5899    | \n",
      "21      | 0.7966    | 0.588     | \n",
      "22      | 0.7972    | 0.5888    | \n",
      "23      | 0.8089    | 0.5901    | \n",
      "24      | 0.8091    | 0.5873    | \n",
      "25      | 0.8045    | 0.5848    | \n",
      "26      | 0.8123    | 0.584     | \n",
      "27      | 0.8169    | 0.5921    | \n",
      "28      | 0.8136    | 0.5913    | \n",
      "29      | 0.8177    | 0.5937    | \n",
      "30      | 0.819     | 0.5939    | \n",
      "31      | 0.826     | 0.5949    | \n",
      "32      | 0.8231    | 0.5984    | \n",
      "33      | 0.8255    | 0.592     | \n",
      "34      | 0.8214    | 0.5909    | \n",
      "35      | 0.822     | 0.5889    | \n",
      "36      | 0.8274    | 0.589     | \n",
      "37      | 0.8308    | 0.5928    | \n",
      "38      | 0.824     | 0.5875    | \n",
      "39      | 0.8394    | 0.5861    | \n",
      "40      | 0.8475    | 0.595     | \n",
      "41      | 0.8508    | 0.5944    | \n",
      "42      | 0.8458    | 0.5923    | \n",
      "43      | 0.8492    | 0.5909    | \n",
      "44      | 0.8498    | 0.5878    | \n",
      "45      | 0.8604    | 0.5892    | \n",
      "46      | 0.8621    | 0.5905    | \n",
      "47      | 0.8627    | 0.5925    | \n",
      "48      | 0.8629    | 0.5929    | \n",
      "49      | 0.8648    | 0.5973    | \n",
      "50      | 0.8631    | 0.5926    | \n",
      "51      | 0.8641    | 0.5913    | \n",
      "52      | 0.8696    | 0.5912    | \n",
      "53      | 0.87      | 0.5933    | \n",
      "54      | 0.8663    | 0.5937    | \n",
      "55      | 0.8682    | 0.5958    | \n",
      "56      | 0.8737    | 0.5952    | \n",
      "57      | 0.874     | 0.5948    | \n",
      "58      | 0.8738    | 0.5907    | \n",
      "59      | 0.8717    | 0.5945    | \n",
      "60      | 0.8763    | 0.5953    | \n",
      "61      | 0.8839    | 0.5961    | \n",
      "62      | 0.8827    | 0.5948    | \n",
      "63      | 0.8846    | 0.5937    | \n",
      "64      | 0.8958    | 0.5942    | \n",
      "65      | 0.8983    | 0.5976    | \n",
      "66      | 0.8977    | 0.5942    | \n",
      "67      | 0.8982    | 0.5924    | \n",
      "68      | 0.8982    | 0.5969    | \n",
      "69      | 0.9039    | 0.5937    | \n",
      "70      | 0.9044    | 0.5966    | \n",
      "71      | 0.9043    | 0.5957    | \n",
      "72      | 0.9044    | 0.594     | \n",
      "73      | 0.9074    | 0.5935    | \n",
      "74      | 0.9076    | 0.595     | \n",
      "75      | 0.9159    | 0.5943    | \n",
      "76      | 0.9161    | 0.593     | \n",
      "77      | 0.915     | 0.5913    | \n",
      "78      | 0.9153    | 0.5925    | \n",
      "79      | 0.9171    | 0.5911    | \n",
      "80      | 0.9171    | 0.5892    | \n",
      "81      | 0.9171    | 0.5903    | \n",
      "82      | 0.9228    | 0.589     | \n",
      "83      | 0.9228    | 0.5896    | \n",
      "84      | 0.9232    | 0.589     | \n",
      "85      | 0.9232    | 0.589     | \n",
      "86      | 0.9197    | 0.59      | \n",
      "87      | 0.9168    | 0.5932    | \n",
      "88      | 0.921     | 0.5936    | \n",
      "89      | 0.924     | 0.5946    | \n",
      "90      | 0.9277    | 0.5914    | \n",
      "91      | 0.9277    | 0.5956    | \n",
      "92      | 0.9277    | 0.5935    | \n",
      "93      | 0.9277    | 0.5944    | \n",
      "94      | 0.9277    | 0.5951    | \n",
      "95      | 0.9279    | 0.5972    | \n",
      "96      | 0.9279    | 0.5968    | \n",
      "97      | 0.9281    | 0.5969    | \n",
      "98      | 0.9318    | 0.5949    | \n",
      "99      | 0.9318    | 0.5968    | \n",
      "100     | 0.9281    | 0.5953    | \n",
      "101     | 0.9254    | 0.5947    | \n",
      "102     | 0.9319    | 0.595     | \n",
      "103     | 0.9319    | 0.5939    | \n",
      "104     | 0.932     | 0.595     | \n",
      "105     | 0.932     | 0.598     | \n",
      "106     | 0.932     | 0.6006    | \n",
      "107     | 0.9363    | 0.6037    | \n",
      "108     | 0.9363    | 0.6048    | \n",
      "109     | 0.9363    | 0.604     | \n",
      "110     | 0.9366    | 0.6033    | \n",
      "111     | 0.9366    | 0.6031    | \n",
      "112     | 0.9403    | 0.604     | \n",
      "113     | 0.9403    | 0.6028    | \n",
      "114     | 0.9404    | 0.6032    | \n",
      "115     | 0.9372    | 0.6008    | \n",
      "116     | 0.9414    | 0.5991    | \n",
      "117     | 0.9422    | 0.6028    | \n",
      "118     | 0.9422    | 0.6036    | \n",
      "119     | 0.9426    | 0.6033    | \n",
      "120     | 0.9426    | 0.6016    | \n",
      "121     | 0.9431    | 0.6021    | \n",
      "122     | 0.9431    | 0.6022    | \n",
      "123     | 0.9431    | 0.6013    | \n",
      "124     | 0.9431    | 0.6006    | \n",
      "125     | 0.9394    | 0.5997    | \n",
      "126     | 0.9431    | 0.5998    | \n",
      "127     | 0.9431    | 0.5989    | \n",
      "128     | 0.9431    | 0.5991    | \n",
      "129     | 0.9471    | 0.5993    | \n",
      "130     | 0.9471    | 0.6001    | \n",
      "131     | 0.9471    | 0.5996    | \n",
      "132     | 0.9471    | 0.5985    | \n",
      "133     | 0.9508    | 0.6004    | \n",
      "134     | 0.9508    | 0.5981    | \n",
      "135     | 0.9508    | 0.5999    | \n",
      "136     | 0.9508    | 0.6003    | \n",
      "137     | 0.9508    | 0.5982    | \n",
      "138     | 0.9484    | 0.6005    | \n",
      "139     | 0.9521    | 0.5994    | \n",
      "140     | 0.9521    | 0.5985    | \n",
      "141     | 0.9521    | 0.5979    | \n",
      "142     | 0.9521    | 0.5981    | \n",
      "143     | 0.9524    | 0.5984    | \n",
      "144     | 0.9524    | 0.5975    | \n",
      "145     | 0.9524    | 0.596     | \n",
      "146     | 0.9567    | 0.5952    | \n",
      "147     | 0.9549    | 0.5972    | \n",
      "148     | 0.9549    | 0.5978    | \n",
      "149     | 0.9532    | 0.5985    | \n",
      "150     | 0.9569    | 0.5963    | \n",
      "151     | 0.9553    | 0.5978    | \n",
      "152     | 0.9595    | 0.5963    | \n",
      "153     | 0.9613    | 0.5943    | \n",
      "154     | 0.9613    | 0.5922    | \n",
      "155     | 0.9613    | 0.5933    | \n",
      "156     | 0.9615    | 0.5949    | \n",
      "157     | 0.9615    | 0.5948    | \n",
      "158     | 0.9615    | 0.5976    | \n",
      "159     | 0.9638    | 0.5982    | \n",
      "160     | 0.9638    | 0.5993    | \n",
      "161     | 0.9638    | 0.5987    | \n",
      "162     | 0.9638    | 0.5993    | \n",
      "163     | 0.9638    | 0.5987    | \n",
      "164     | 0.9638    | 0.5984    | \n",
      "165     | 0.9638    | 0.599     | \n",
      "166     | 0.9638    | 0.5975    | \n",
      "167     | 0.9642    | 0.5984    | \n",
      "168     | 0.9679    | 0.5977    | \n",
      "169     | 0.968     | 0.5988    | \n",
      "170     | 0.968     | 0.5974    | \n",
      "171     | 0.968     | 0.5993    | \n",
      "172     | 0.9717    | 0.596     | \n",
      "173     | 0.9717    | 0.5955    | \n",
      "174     | 0.9719    | 0.5955    | \n",
      "175     | 0.9719    | 0.5975    | \n",
      "176     | 0.972     | 0.5953    | \n",
      "177     | 0.972     | 0.5944    | \n",
      "178     | 0.972     | 0.5965    | \n",
      "179     | 0.972     | 0.5961    | \n",
      "180     | 0.972     | 0.5964    | \n",
      "181     | 0.972     | 0.5967    | \n",
      "182     | 0.972     | 0.5942    | \n",
      "183     | 0.972     | 0.5936    | \n",
      "184     | 0.972     | 0.5929    | \n",
      "185     | 0.972     | 0.5947    | \n",
      "186     | 0.9721    | 0.595     | \n",
      "187     | 0.9721    | 0.595     | \n",
      "188     | 0.9721    | 0.595     | \n",
      "189     | 0.9721    | 0.598     | \n",
      "190     | 0.9721    | 0.5975    | \n",
      "191     | 0.9721    | 0.598     | \n",
      "192     | 0.9721    | 0.5991    | \n",
      "193     | 0.9721    | 0.5954    | \n",
      "194     | 0.9721    | 0.5963    | \n",
      "195     | 0.9721    | 0.5972    | \n",
      "196     | 0.9721    | 0.5981    | \n",
      "197     | 0.9721    | 0.5954    | \n",
      "198     | 0.972     | 0.5991    | \n",
      "199     | 0.972     | 0.5969    | \n",
      "200     | 0.972     | 0.5953    | \n",
      "201     | 0.972     | 0.5956    | \n",
      "202     | 0.972     | 0.5954    | \n",
      "203     | 0.972     | 0.5961    | \n",
      "204     | 0.972     | 0.5975    | \n",
      "205     | 0.972     | 0.5979    | \n",
      "206     | 0.972     | 0.5983    | \n",
      "207     | 0.9719    | 0.5977    | \n",
      "208     | 0.9719    | 0.5979    | \n",
      "209     | 0.9719    | 0.596     | \n",
      "---------------------------------\n",
      "Finished sucessfully.\n",
      "NDCG@10 on training data: 0.9363\n",
      "NDCG@10 on validation data: 0.6048\n",
      "---------------------------------\n",
      "ERR@10 on test data: 0.2249\n",
      "\n",
      "Model saved to: ../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2020_LambdaMart.txt\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -train ../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2020.svmrank -test ../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2021.svmrank -tvs .2 -ranker 6 -metric2t NDCG@10 -metric2T ERR@10 -save ../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2020_LambdaMart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Training data:\t../Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank\n",
      "Test data:\t../Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank\n",
      "Train-Validation split: 0.2\n",
      "Feature vector representation: Dense.\n",
      "Ranking method:\tLambdaMART\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@10\n",
      "Test metric:\tERR@10\n",
      "Highest relevance label (to compute ERR): 4\n",
      "Feature normalization: No\n",
      "Model file: ../Data/trained_models_2020_2021/sbert_models/sbert_model_2020_LambdaMart.txt\n",
      "\n",
      "[+] LambdaMART's Parameters:\n",
      "No. of trees: 1000\n",
      "No. of leaves: 10\n",
      "No. of threshold candidates: 256\n",
      "Min leaf support: 1\n",
      "Learning rate: 0.1\n",
      "Stop early: 100 rounds without performance gain on validation data\n",
      "\n",
      "Reading feature file [../Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank]... [Done.]            \n",
      "(49 ranked lists, 3612 entries read)\n",
      "Reading feature file [../Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank]... [Done.]            \n",
      "(50 ranked lists, 6884 entries read)\n",
      "Initializing... [Done]\n",
      "---------------------------------\n",
      "Training starts...\n",
      "---------------------------------\n",
      "#iter   | NDCG@10-T | NDCG@10-V | \n",
      "---------------------------------\n",
      "1       | 0.5682    | 0.4316    | \n",
      "2       | 0.6602    | 0.4313    | \n",
      "3       | 0.6935    | 0.4251    | \n",
      "4       | 0.6935    | 0.4124    | \n",
      "5       | 0.6935    | 0.4116    | \n",
      "6       | 0.6922    | 0.4124    | \n",
      "7       | 0.7096    | 0.4068    | \n",
      "8       | 0.7119    | 0.4075    | \n",
      "9       | 0.7132    | 0.4073    | \n",
      "10      | 0.7132    | 0.4048    | \n",
      "11      | 0.7132    | 0.4085    | \n",
      "12      | 0.7123    | 0.4061    | \n",
      "13      | 0.7221    | 0.4055    | \n",
      "14      | 0.7269    | 0.4095    | \n",
      "15      | 0.7304    | 0.4135    | \n",
      "16      | 0.7337    | 0.4037    | \n",
      "17      | 0.7348    | 0.4014    | \n",
      "18      | 0.7347    | 0.397     | \n",
      "19      | 0.7396    | 0.4032    | \n",
      "20      | 0.7399    | 0.4014    | \n",
      "21      | 0.7449    | 0.3957    | \n",
      "22      | 0.7432    | 0.3968    | \n",
      "23      | 0.7368    | 0.4027    | \n",
      "24      | 0.7435    | 0.4042    | \n",
      "25      | 0.7503    | 0.4084    | \n",
      "26      | 0.756     | 0.4031    | \n",
      "27      | 0.7575    | 0.403     | \n",
      "28      | 0.7638    | 0.4058    | \n",
      "29      | 0.7614    | 0.4083    | \n",
      "30      | 0.7648    | 0.4077    | \n",
      "31      | 0.7677    | 0.4127    | \n",
      "32      | 0.7657    | 0.4173    | \n",
      "33      | 0.7781    | 0.4183    | \n",
      "34      | 0.7811    | 0.4168    | \n",
      "35      | 0.7869    | 0.4206    | \n",
      "36      | 0.7816    | 0.418     | \n",
      "37      | 0.7869    | 0.4223    | \n",
      "38      | 0.7924    | 0.4254    | \n",
      "39      | 0.7894    | 0.4241    | \n",
      "40      | 0.8004    | 0.4238    | \n",
      "41      | 0.8024    | 0.4275    | \n",
      "42      | 0.8032    | 0.4269    | \n",
      "43      | 0.802     | 0.4295    | \n",
      "44      | 0.8017    | 0.4297    | \n",
      "45      | 0.8052    | 0.4296    | \n",
      "46      | 0.8089    | 0.4293    | \n",
      "47      | 0.8126    | 0.4327    | \n",
      "48      | 0.819     | 0.4329    | \n",
      "49      | 0.819     | 0.4361    | \n",
      "50      | 0.8257    | 0.4332    | \n",
      "51      | 0.8258    | 0.4348    | \n",
      "52      | 0.8357    | 0.4349    | \n",
      "53      | 0.8304    | 0.4386    | \n",
      "54      | 0.8388    | 0.4375    | \n",
      "55      | 0.8393    | 0.4369    | \n",
      "56      | 0.8439    | 0.4408    | \n",
      "57      | 0.8486    | 0.4361    | \n",
      "58      | 0.8492    | 0.4345    | \n",
      "59      | 0.8494    | 0.4363    | \n",
      "60      | 0.8484    | 0.4354    | \n",
      "61      | 0.8502    | 0.4348    | \n",
      "62      | 0.8502    | 0.4389    | \n",
      "63      | 0.8484    | 0.442     | \n",
      "64      | 0.8523    | 0.4394    | \n",
      "65      | 0.8523    | 0.4378    | \n",
      "66      | 0.8559    | 0.4412    | \n",
      "67      | 0.8533    | 0.4408    | \n",
      "68      | 0.8561    | 0.4421    | \n",
      "69      | 0.8565    | 0.442     | \n",
      "70      | 0.8566    | 0.447     | \n",
      "71      | 0.8637    | 0.4471    | \n",
      "72      | 0.8638    | 0.4486    | \n",
      "73      | 0.8675    | 0.449     | \n",
      "74      | 0.878     | 0.4481    | \n",
      "75      | 0.8763    | 0.4478    | \n",
      "76      | 0.8845    | 0.448     | \n",
      "77      | 0.8848    | 0.4544    | \n",
      "78      | 0.8848    | 0.4524    | \n",
      "79      | 0.885     | 0.4545    | \n",
      "80      | 0.885     | 0.4528    | \n",
      "81      | 0.8858    | 0.4521    | \n",
      "82      | 0.8864    | 0.4487    | \n",
      "83      | 0.8868    | 0.4477    | \n",
      "84      | 0.8853    | 0.4475    | \n",
      "85      | 0.8871    | 0.4479    | \n",
      "86      | 0.8876    | 0.4479    | \n",
      "87      | 0.8857    | 0.4471    | \n",
      "88      | 0.8935    | 0.4451    | \n",
      "89      | 0.8935    | 0.4474    | \n",
      "90      | 0.8937    | 0.4476    | \n",
      "91      | 0.8937    | 0.4401    | \n",
      "92      | 0.8975    | 0.4398    | \n",
      "93      | 0.8977    | 0.4383    | \n",
      "94      | 0.8978    | 0.4339    | \n",
      "95      | 0.902     | 0.4332    | \n",
      "96      | 0.9067    | 0.4349    | \n",
      "97      | 0.9107    | 0.4383    | \n",
      "98      | 0.9113    | 0.437     | \n",
      "99      | 0.9113    | 0.4335    | \n",
      "100     | 0.9149    | 0.4346    | \n",
      "101     | 0.9151    | 0.4313    | \n",
      "102     | 0.9151    | 0.4318    | \n",
      "103     | 0.9134    | 0.431     | \n",
      "104     | 0.9155    | 0.4317    | \n",
      "105     | 0.9155    | 0.4325    | \n",
      "106     | 0.9194    | 0.4328    | \n",
      "107     | 0.9195    | 0.4274    | \n",
      "108     | 0.9195    | 0.4312    | \n",
      "109     | 0.9195    | 0.4254    | \n",
      "110     | 0.9195    | 0.4271    | \n",
      "111     | 0.9199    | 0.4266    | \n",
      "112     | 0.9199    | 0.4272    | \n",
      "113     | 0.9237    | 0.426     | \n",
      "114     | 0.9237    | 0.4269    | \n",
      "115     | 0.9237    | 0.4242    | \n",
      "116     | 0.9239    | 0.4238    | \n",
      "117     | 0.9239    | 0.4218    | \n",
      "118     | 0.9239    | 0.42      | \n",
      "119     | 0.9239    | 0.4219    | \n",
      "120     | 0.9239    | 0.4212    | \n",
      "121     | 0.9239    | 0.4208    | \n",
      "122     | 0.9239    | 0.4209    | \n",
      "123     | 0.9275    | 0.4233    | \n",
      "124     | 0.9278    | 0.426     | \n",
      "125     | 0.9315    | 0.4243    | \n",
      "126     | 0.9315    | 0.4264    | \n",
      "127     | 0.9315    | 0.4313    | \n",
      "128     | 0.9315    | 0.4276    | \n",
      "129     | 0.932     | 0.4281    | \n",
      "130     | 0.932     | 0.4286    | \n",
      "131     | 0.932     | 0.4278    | \n",
      "132     | 0.932     | 0.4293    | \n",
      "133     | 0.932     | 0.4303    | \n",
      "134     | 0.932     | 0.4289    | \n",
      "135     | 0.932     | 0.4325    | \n",
      "136     | 0.9339    | 0.4315    | \n",
      "137     | 0.9349    | 0.43      | \n",
      "138     | 0.9353    | 0.4307    | \n",
      "139     | 0.9353    | 0.4322    | \n",
      "140     | 0.9357    | 0.4325    | \n",
      "141     | 0.9357    | 0.4321    | \n",
      "142     | 0.9357    | 0.4278    | \n",
      "143     | 0.9357    | 0.4269    | \n",
      "144     | 0.9357    | 0.4265    | \n",
      "145     | 0.9393    | 0.4277    | \n",
      "146     | 0.9393    | 0.4269    | \n",
      "147     | 0.9393    | 0.4279    | \n",
      "148     | 0.9395    | 0.4276    | \n",
      "149     | 0.9395    | 0.429     | \n",
      "150     | 0.9398    | 0.4302    | \n",
      "151     | 0.9398    | 0.4289    | \n",
      "152     | 0.9475    | 0.4345    | \n",
      "153     | 0.9476    | 0.4366    | \n",
      "154     | 0.9476    | 0.4363    | \n",
      "155     | 0.9513    | 0.4362    | \n",
      "156     | 0.9553    | 0.4326    | \n",
      "157     | 0.9553    | 0.4335    | \n",
      "158     | 0.9589    | 0.4292    | \n",
      "159     | 0.9626    | 0.4318    | \n",
      "160     | 0.9626    | 0.4351    | \n",
      "161     | 0.9626    | 0.4293    | \n",
      "162     | 0.9645    | 0.4287    | \n",
      "163     | 0.9645    | 0.4259    | \n",
      "164     | 0.9645    | 0.4231    | \n",
      "165     | 0.9647    | 0.4249    | \n",
      "166     | 0.9647    | 0.4256    | \n",
      "167     | 0.9647    | 0.4222    | \n",
      "168     | 0.9647    | 0.4209    | \n",
      "169     | 0.9647    | 0.4187    | \n",
      "170     | 0.9647    | 0.4136    | \n",
      "171     | 0.9647    | 0.4141    | \n",
      "172     | 0.9647    | 0.4162    | \n",
      "173     | 0.9647    | 0.4208    | \n",
      "174     | 0.9647    | 0.4243    | \n",
      "175     | 0.9647    | 0.4238    | \n",
      "176     | 0.9647    | 0.423     | \n",
      "177     | 0.9685    | 0.4255    | \n",
      "178     | 0.9685    | 0.4285    | \n",
      "179     | 0.9685    | 0.426     | \n",
      "180     | 0.9685    | 0.4223    | \n",
      "---------------------------------\n",
      "Finished sucessfully.\n",
      "NDCG@10 on training data: 0.885\n",
      "NDCG@10 on validation data: 0.4545\n",
      "---------------------------------\n",
      "ERR@10 on test data: 0.1797\n",
      "\n",
      "Model saved to: ../Data/trained_models_2020_2021/sbert_models/sbert_model_2020_LambdaMart.txt\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -train ../Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank -test ../Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank -tvs .2 -ranker 6 -metric2t NDCG@10 -metric2T ERR@10 -save ../Data/trained_models_2020_2021/sbert_models/sbert_model_2020_LambdaMart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Training data:\t../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank\n",
      "Test data:\t../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank\n",
      "Train-Validation split: 0.2\n",
      "Feature vector representation: Dense.\n",
      "Ranking method:\tLambdaMART\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@10\n",
      "Test metric:\tERR@10\n",
      "Highest relevance label (to compute ERR): 4\n",
      "Feature normalization: No\n",
      "Model file: ../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2020_LambdaMart.txt\n",
      "\n",
      "[+] LambdaMART's Parameters:\n",
      "No. of trees: 1000\n",
      "No. of leaves: 10\n",
      "No. of threshold candidates: 256\n",
      "Min leaf support: 1\n",
      "Learning rate: 0.1\n",
      "Stop early: 100 rounds without performance gain on validation data\n",
      "\n",
      "Reading feature file [../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank]... [Done.]            \n",
      "(49 ranked lists, 1806 entries read)\n",
      "Reading feature file [../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank]... [Done.]            \n",
      "(50 ranked lists, 3442 entries read)\n",
      "Initializing... [Done]\n",
      "---------------------------------\n",
      "Training starts...\n",
      "---------------------------------\n",
      "#iter   | NDCG@10-T | NDCG@10-V | \n",
      "---------------------------------\n",
      "1       | 0.6394    | 0.5004    | \n",
      "2       | 0.6871    | 0.5293    | \n",
      "3       | 0.7361    | 0.5536    | \n",
      "4       | 0.7286    | 0.5529    | \n",
      "5       | 0.7337    | 0.5549    | \n",
      "6       | 0.7295    | 0.5464    | \n",
      "7       | 0.7482    | 0.5491    | \n",
      "8       | 0.7463    | 0.55      | \n",
      "9       | 0.7496    | 0.557     | \n",
      "10      | 0.7544    | 0.5599    | \n",
      "11      | 0.7546    | 0.564     | \n",
      "12      | 0.7505    | 0.5659    | \n",
      "13      | 0.7657    | 0.5638    | \n",
      "14      | 0.7673    | 0.5596    | \n",
      "15      | 0.7801    | 0.562     | \n",
      "16      | 0.775     | 0.5615    | \n",
      "17      | 0.7718    | 0.5586    | \n",
      "18      | 0.7629    | 0.5595    | \n",
      "19      | 0.7638    | 0.5626    | \n",
      "20      | 0.766     | 0.5641    | \n",
      "21      | 0.7758    | 0.5623    | \n",
      "22      | 0.7803    | 0.5595    | \n",
      "23      | 0.7757    | 0.5657    | \n",
      "24      | 0.7863    | 0.5703    | \n",
      "25      | 0.7888    | 0.5667    | \n",
      "26      | 0.7901    | 0.5663    | \n",
      "27      | 0.7923    | 0.5665    | \n",
      "28      | 0.7985    | 0.5697    | \n",
      "29      | 0.8005    | 0.5658    | \n",
      "30      | 0.8008    | 0.5687    | \n",
      "31      | 0.8008    | 0.573     | \n",
      "32      | 0.8007    | 0.5687    | \n",
      "33      | 0.7995    | 0.5702    | \n",
      "34      | 0.8012    | 0.5674    | \n",
      "35      | 0.8047    | 0.5685    | \n",
      "36      | 0.8011    | 0.5672    | \n",
      "37      | 0.8012    | 0.5614    | \n",
      "38      | 0.8011    | 0.5628    | \n",
      "39      | 0.7995    | 0.5606    | \n",
      "40      | 0.7957    | 0.5627    | \n",
      "41      | 0.7958    | 0.564     | \n",
      "42      | 0.7964    | 0.565     | \n",
      "43      | 0.7993    | 0.5641    | \n",
      "44      | 0.7996    | 0.5614    | \n",
      "45      | 0.8039    | 0.5665    | \n",
      "46      | 0.8097    | 0.5622    | \n",
      "47      | 0.8135    | 0.5595    | \n",
      "48      | 0.8138    | 0.5607    | \n",
      "49      | 0.8223    | 0.5569    | \n",
      "50      | 0.8199    | 0.5567    | \n",
      "51      | 0.8267    | 0.5575    | \n",
      "52      | 0.8252    | 0.5558    | \n",
      "53      | 0.8262    | 0.5573    | \n",
      "54      | 0.8218    | 0.5567    | \n",
      "55      | 0.824     | 0.5579    | \n",
      "56      | 0.8244    | 0.5584    | \n",
      "57      | 0.8251    | 0.5571    | \n",
      "58      | 0.8248    | 0.5548    | \n",
      "59      | 0.821     | 0.5585    | \n",
      "60      | 0.8342    | 0.5588    | \n",
      "61      | 0.8351    | 0.5576    | \n",
      "62      | 0.8375    | 0.5606    | \n",
      "63      | 0.8383    | 0.5607    | \n",
      "64      | 0.8385    | 0.5573    | \n",
      "65      | 0.8391    | 0.5549    | \n",
      "66      | 0.8411    | 0.5544    | \n",
      "67      | 0.8401    | 0.554     | \n",
      "68      | 0.8475    | 0.5533    | \n",
      "69      | 0.8639    | 0.5553    | \n",
      "70      | 0.8753    | 0.5579    | \n",
      "71      | 0.8753    | 0.5578    | \n",
      "72      | 0.875     | 0.5559    | \n",
      "73      | 0.8771    | 0.5525    | \n",
      "74      | 0.876     | 0.5533    | \n",
      "75      | 0.8789    | 0.5521    | \n",
      "76      | 0.8818    | 0.5523    | \n",
      "77      | 0.8818    | 0.5516    | \n",
      "78      | 0.8873    | 0.5501    | \n",
      "79      | 0.878     | 0.5476    | \n",
      "80      | 0.881     | 0.5508    | \n",
      "81      | 0.8807    | 0.5519    | \n",
      "82      | 0.8872    | 0.5497    | \n",
      "83      | 0.8779    | 0.5481    | \n",
      "84      | 0.8783    | 0.5483    | \n",
      "85      | 0.8782    | 0.5465    | \n",
      "86      | 0.8784    | 0.545     | \n",
      "87      | 0.8769    | 0.5469    | \n",
      "88      | 0.8827    | 0.547     | \n",
      "89      | 0.8852    | 0.5447    | \n",
      "90      | 0.8817    | 0.5465    | \n",
      "91      | 0.8841    | 0.5498    | \n",
      "92      | 0.8828    | 0.5521    | \n",
      "93      | 0.8837    | 0.5464    | \n",
      "94      | 0.8833    | 0.546     | \n",
      "95      | 0.8811    | 0.544     | \n",
      "96      | 0.884     | 0.5435    | \n",
      "97      | 0.8846    | 0.5406    | \n",
      "98      | 0.8844    | 0.5417    | \n",
      "99      | 0.8851    | 0.5397    | \n",
      "100     | 0.8845    | 0.5397    | \n",
      "101     | 0.8884    | 0.5396    | \n",
      "102     | 0.8919    | 0.5383    | \n",
      "103     | 0.8918    | 0.5395    | \n",
      "104     | 0.8975    | 0.5392    | \n",
      "105     | 0.9001    | 0.539     | \n",
      "106     | 0.8991    | 0.5416    | \n",
      "107     | 0.9004    | 0.5399    | \n",
      "108     | 0.8972    | 0.5424    | \n",
      "109     | 0.8982    | 0.5411    | \n",
      "110     | 0.8974    | 0.5415    | \n",
      "111     | 0.8989    | 0.5412    | \n",
      "112     | 0.9041    | 0.5393    | \n",
      "113     | 0.8988    | 0.5392    | \n",
      "114     | 0.9041    | 0.5373    | \n",
      "115     | 0.905     | 0.5375    | \n",
      "116     | 0.9068    | 0.5395    | \n",
      "117     | 0.9072    | 0.5391    | \n",
      "118     | 0.9042    | 0.5401    | \n",
      "119     | 0.9074    | 0.5397    | \n",
      "120     | 0.9074    | 0.5421    | \n",
      "121     | 0.908     | 0.5431    | \n",
      "122     | 0.9082    | 0.5444    | \n",
      "123     | 0.9084    | 0.5439    | \n",
      "124     | 0.9066    | 0.5454    | \n",
      "125     | 0.9106    | 0.5443    | \n",
      "126     | 0.9093    | 0.5456    | \n",
      "127     | 0.9138    | 0.5427    | \n",
      "128     | 0.9175    | 0.5433    | \n",
      "129     | 0.9175    | 0.5436    | \n",
      "130     | 0.9187    | 0.5436    | \n",
      "131     | 0.9191    | 0.5471    | \n",
      "132     | 0.9181    | 0.5463    | \n",
      "---------------------------------\n",
      "Finished sucessfully.\n",
      "NDCG@10 on training data: 0.8008\n",
      "NDCG@10 on validation data: 0.573\n",
      "---------------------------------\n",
      "ERR@10 on test data: 0.1789\n",
      "\n",
      "Model saved to: ../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2020_LambdaMart.txt\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -train ../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank -test ../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank -tvs .2 -ranker 6 -metric2t NDCG@10 -metric2T ERR@10 -save ../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2020_LambdaMart.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank 2021 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Model file:\t../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2020.txt\n",
      "Feature normalization: No\n",
      "Model:\t\tRandom Forests\n",
      "Reading feature file [../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2021.svmrank]... [Done.]            \n",
      "(50 ranked lists, 3442 entries read)\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -load ../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2020.txt -rank ../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2021.svmrank -score ../Data/scorefiles_2020_2021/linguistic_sentiment_scorefiles/argument2021_scorefile_LambdaMart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Model file:\t../Data/trained_models_2020_2021/sbert_models/sbert_model_2020.txt\n",
      "Feature normalization: No\n",
      "Model:\t\tRandom Forests\n",
      "Reading feature file [../Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank]... [Done.]            \n",
      "(50 ranked lists, 6884 entries read)\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -load ../Data/trained_models_2020_2021/sbert_models/sbert_model_2020.txt -rank ../Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank -score ../Data/scorefiles_2020_2021/sbert_scorefiles/argument2021_scorefile_LambdaMart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Model file:\t../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2020.txt\n",
      "Feature normalization: No\n",
      "Model:\t\tRandom Forests\n",
      "Reading feature file [../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank]... [Done.]            \n",
      "(50 ranked lists, 3442 entries read)\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -load ../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2020.txt -rank ../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank -score ../Data/scorefiles_2020_2021/sentiment_sarcasm_scorefiles/argument2021_scorefile_LambdaMart.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 2021 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Training data:\t../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2021.svmrank\n",
      "Test data:\t../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2020.svmrank\n",
      "Train-Validation split: 0.2\n",
      "Feature vector representation: Dense.\n",
      "Ranking method:\tLambdaMART\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@10\n",
      "Test metric:\tERR@10\n",
      "Highest relevance label (to compute ERR): 4\n",
      "Feature normalization: No\n",
      "Model file: ../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2021__LambdaMart.txt\n",
      "\n",
      "[+] LambdaMART's Parameters:\n",
      "No. of trees: 1000\n",
      "No. of leaves: 10\n",
      "No. of threshold candidates: 256\n",
      "Min leaf support: 1\n",
      "Learning rate: 0.1\n",
      "Stop early: 100 rounds without performance gain on validation data\n",
      "\n",
      "Reading feature file [../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2021.svmrank]... [Done.]            \n",
      "(50 ranked lists, 3442 entries read)\n",
      "Reading feature file [../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2020.svmrank]... [Done.]            \n",
      "(49 ranked lists, 1806 entries read)\n",
      "Initializing... [Done]\n",
      "---------------------------------\n",
      "Training starts...\n",
      "---------------------------------\n",
      "#iter   | NDCG@10-T | NDCG@10-V | \n",
      "---------------------------------\n",
      "1       | 0.4751    | 0.4632    | \n",
      "2       | 0.6464    | 0.5549    | \n",
      "3       | 0.6599    | 0.5005    | \n",
      "4       | 0.6915    | 0.525     | \n",
      "5       | 0.6936    | 0.5323    | \n",
      "6       | 0.6936    | 0.5205    | \n",
      "7       | 0.6928    | 0.5101    | \n",
      "8       | 0.6915    | 0.5226    | \n",
      "9       | 0.6986    | 0.5124    | \n",
      "10      | 0.7072    | 0.5021    | \n",
      "11      | 0.7194    | 0.5017    | \n",
      "12      | 0.7222    | 0.5046    | \n",
      "13      | 0.7177    | 0.5216    | \n",
      "14      | 0.7186    | 0.5169    | \n",
      "15      | 0.7267    | 0.5303    | \n",
      "16      | 0.7181    | 0.5272    | \n",
      "17      | 0.7459    | 0.5339    | \n",
      "18      | 0.7478    | 0.5355    | \n",
      "19      | 0.746     | 0.5407    | \n",
      "20      | 0.7624    | 0.5366    | \n",
      "21      | 0.7529    | 0.5383    | \n",
      "22      | 0.7694    | 0.5453    | \n",
      "23      | 0.7692    | 0.5483    | \n",
      "24      | 0.773     | 0.5409    | \n",
      "25      | 0.7694    | 0.54      | \n",
      "26      | 0.7719    | 0.5392    | \n",
      "27      | 0.7752    | 0.5457    | \n",
      "28      | 0.7737    | 0.555     | \n",
      "29      | 0.7876    | 0.5525    | \n",
      "30      | 0.785     | 0.5491    | \n",
      "31      | 0.7858    | 0.5468    | \n",
      "32      | 0.7874    | 0.5497    | \n",
      "33      | 0.8023    | 0.551     | \n",
      "34      | 0.7981    | 0.5518    | \n",
      "35      | 0.8026    | 0.557     | \n",
      "36      | 0.8043    | 0.5625    | \n",
      "37      | 0.8288    | 0.5566    | \n",
      "38      | 0.8298    | 0.5589    | \n",
      "39      | 0.8406    | 0.5662    | \n",
      "40      | 0.8487    | 0.5665    | \n",
      "41      | 0.8478    | 0.5661    | \n",
      "42      | 0.8488    | 0.5654    | \n",
      "43      | 0.8491    | 0.5692    | \n",
      "44      | 0.8519    | 0.5737    | \n",
      "45      | 0.8577    | 0.5766    | \n",
      "46      | 0.8753    | 0.573     | \n",
      "47      | 0.8773    | 0.5728    | \n",
      "48      | 0.8755    | 0.5792    | \n",
      "49      | 0.8855    | 0.5813    | \n",
      "50      | 0.8772    | 0.5848    | \n",
      "51      | 0.8864    | 0.5896    | \n",
      "52      | 0.8845    | 0.5881    | \n",
      "53      | 0.8962    | 0.5884    | \n",
      "54      | 0.8923    | 0.5911    | \n",
      "55      | 0.8928    | 0.5846    | \n",
      "56      | 0.8943    | 0.586     | \n",
      "57      | 0.8947    | 0.5873    | \n",
      "58      | 0.8991    | 0.5882    | \n",
      "59      | 0.9034    | 0.5892    | \n",
      "60      | 0.91      | 0.5805    | \n",
      "61      | 0.9111    | 0.5832    | \n",
      "62      | 0.9065    | 0.5845    | \n",
      "63      | 0.9134    | 0.5787    | \n",
      "64      | 0.9119    | 0.5821    | \n",
      "65      | 0.9163    | 0.5787    | \n",
      "66      | 0.9165    | 0.5813    | \n",
      "67      | 0.9155    | 0.5822    | \n",
      "68      | 0.925     | 0.5772    | \n",
      "69      | 0.9256    | 0.5773    | \n",
      "70      | 0.9232    | 0.5772    | \n",
      "71      | 0.9192    | 0.5797    | \n",
      "72      | 0.9271    | 0.574     | \n",
      "73      | 0.9322    | 0.5759    | \n",
      "74      | 0.9284    | 0.5723    | \n",
      "75      | 0.9326    | 0.5725    | \n",
      "76      | 0.9347    | 0.5684    | \n",
      "77      | 0.9347    | 0.57      | \n",
      "78      | 0.9342    | 0.5714    | \n",
      "79      | 0.93      | 0.5691    | \n",
      "80      | 0.9326    | 0.5719    | \n",
      "81      | 0.9342    | 0.5705    | \n",
      "82      | 0.9406    | 0.5677    | \n",
      "83      | 0.93      | 0.5671    | \n",
      "84      | 0.9406    | 0.5645    | \n",
      "85      | 0.9406    | 0.5647    | \n",
      "86      | 0.943     | 0.5659    | \n",
      "87      | 0.9344    | 0.5691    | \n",
      "88      | 0.9365    | 0.571     | \n",
      "89      | 0.9455    | 0.5716    | \n",
      "90      | 0.9481    | 0.5726    | \n",
      "91      | 0.9481    | 0.5724    | \n",
      "92      | 0.9481    | 0.5698    | \n",
      "93      | 0.9418    | 0.5676    | \n",
      "94      | 0.9439    | 0.5702    | \n",
      "95      | 0.9462    | 0.5732    | \n",
      "96      | 0.9487    | 0.5711    | \n",
      "97      | 0.9423    | 0.573     | \n",
      "98      | 0.9462    | 0.5706    | \n",
      "99      | 0.9462    | 0.5707    | \n",
      "100     | 0.9462    | 0.5715    | \n",
      "101     | 0.9462    | 0.5738    | \n",
      "102     | 0.9462    | 0.5726    | \n",
      "103     | 0.9472    | 0.5719    | \n",
      "104     | 0.9472    | 0.5723    | \n",
      "105     | 0.9472    | 0.5735    | \n",
      "106     | 0.9472    | 0.5774    | \n",
      "107     | 0.9472    | 0.5793    | \n",
      "108     | 0.9544    | 0.5781    | \n",
      "109     | 0.9548    | 0.5757    | \n",
      "110     | 0.9573    | 0.5793    | \n",
      "111     | 0.9582    | 0.5788    | \n",
      "112     | 0.9582    | 0.5787    | \n",
      "113     | 0.9582    | 0.5779    | \n",
      "114     | 0.9582    | 0.5758    | \n",
      "115     | 0.9587    | 0.574     | \n",
      "116     | 0.9651    | 0.5759    | \n",
      "117     | 0.9658    | 0.5755    | \n",
      "118     | 0.9662    | 0.5775    | \n",
      "119     | 0.9662    | 0.5731    | \n",
      "120     | 0.9737    | 0.5761    | \n",
      "121     | 0.9743    | 0.5777    | \n",
      "122     | 0.9743    | 0.577     | \n",
      "123     | 0.9743    | 0.5765    | \n",
      "124     | 0.9701    | 0.5777    | \n",
      "125     | 0.9743    | 0.5773    | \n",
      "126     | 0.9743    | 0.5758    | \n",
      "127     | 0.9743    | 0.5739    | \n",
      "128     | 0.9743    | 0.5744    | \n",
      "129     | 0.9743    | 0.5743    | \n",
      "130     | 0.9743    | 0.5721    | \n",
      "131     | 0.9743    | 0.5721    | \n",
      "132     | 0.9743    | 0.5714    | \n",
      "133     | 0.9743    | 0.5739    | \n",
      "134     | 0.9743    | 0.5745    | \n",
      "135     | 0.9743    | 0.5787    | \n",
      "136     | 0.9743    | 0.5802    | \n",
      "137     | 0.9743    | 0.5805    | \n",
      "138     | 0.9785    | 0.5798    | \n",
      "139     | 0.9785    | 0.578     | \n",
      "140     | 0.9787    | 0.5803    | \n",
      "141     | 0.9787    | 0.5784    | \n",
      "142     | 0.9787    | 0.5821    | \n",
      "143     | 0.9787    | 0.5783    | \n",
      "144     | 0.9787    | 0.5769    | \n",
      "145     | 0.9787    | 0.5812    | \n",
      "146     | 0.9787    | 0.5764    | \n",
      "147     | 0.9787    | 0.5783    | \n",
      "148     | 0.9787    | 0.5818    | \n",
      "149     | 0.9787    | 0.5819    | \n",
      "150     | 0.9787    | 0.5801    | \n",
      "151     | 0.9787    | 0.5793    | \n",
      "152     | 0.9787    | 0.5793    | \n",
      "153     | 0.9787    | 0.5811    | \n",
      "154     | 0.9787    | 0.5795    | \n",
      "155     | 0.9787    | 0.58      | \n",
      "---------------------------------\n",
      "Finished sucessfully.\n",
      "NDCG@10 on training data: 0.8923\n",
      "NDCG@10 on validation data: 0.5911\n",
      "---------------------------------\n",
      "ERR@10 on test data: 1.0044\n",
      "\n",
      "Model saved to: ../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2021__LambdaMart.txt\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -train ../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2021.svmrank -test ../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2020.svmrank -tvs .2 -ranker 6 -metric2t NDCG@10 -metric2T ERR@10 -save ../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2021__LambdaMart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Training data:\t../Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank\n",
      "Test data:\t../Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank\n",
      "Train-Validation split: 0.2\n",
      "Feature vector representation: Dense.\n",
      "Ranking method:\tLambdaMART\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@10\n",
      "Test metric:\tERR@10\n",
      "Highest relevance label (to compute ERR): 4\n",
      "Feature normalization: No\n",
      "Model file: ../Data/trained_models_2020_2021/sbert_models/sbert_model_2021__LambdaMart.txt\n",
      "\n",
      "[+] LambdaMART's Parameters:\n",
      "No. of trees: 1000\n",
      "No. of leaves: 10\n",
      "No. of threshold candidates: 256\n",
      "Min leaf support: 1\n",
      "Learning rate: 0.1\n",
      "Stop early: 100 rounds without performance gain on validation data\n",
      "\n",
      "Reading feature file [../Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank]... [Done.]            \n",
      "(50 ranked lists, 6884 entries read)\n",
      "Reading feature file [../Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank]... [Done.]            \n",
      "(49 ranked lists, 3612 entries read)\n",
      "Initializing... [Done]\n",
      "---------------------------------\n",
      "Training starts...\n",
      "---------------------------------\n",
      "#iter   | NDCG@10-T | NDCG@10-V | \n",
      "---------------------------------\n",
      "1       | 0.4439    | 0.443     | \n",
      "2       | 0.499     | 0.4352    | \n",
      "3       | 0.5005    | 0.4326    | \n",
      "4       | 0.5032    | 0.422     | \n",
      "5       | 0.5244    | 0.4071    | \n",
      "6       | 0.5364    | 0.4017    | \n",
      "7       | 0.5471    | 0.4033    | \n",
      "8       | 0.5704    | 0.3959    | \n",
      "9       | 0.5909    | 0.3729    | \n",
      "10      | 0.5833    | 0.3775    | \n",
      "11      | 0.6179    | 0.3723    | \n",
      "12      | 0.6132    | 0.3802    | \n",
      "13      | 0.6208    | 0.3767    | \n",
      "14      | 0.6132    | 0.3875    | \n",
      "15      | 0.6218    | 0.3776    | \n",
      "16      | 0.6251    | 0.3836    | \n",
      "17      | 0.6332    | 0.377     | \n",
      "18      | 0.6472    | 0.378     | \n",
      "19      | 0.6472    | 0.3704    | \n",
      "20      | 0.6477    | 0.3718    | \n",
      "21      | 0.6477    | 0.374     | \n",
      "22      | 0.6576    | 0.3745    | \n",
      "23      | 0.6763    | 0.3783    | \n",
      "24      | 0.6823    | 0.3839    | \n",
      "25      | 0.6871    | 0.379     | \n",
      "26      | 0.6903    | 0.3918    | \n",
      "27      | 0.6773    | 0.397     | \n",
      "28      | 0.6729    | 0.3965    | \n",
      "29      | 0.7618    | 0.3879    | \n",
      "30      | 0.7506    | 0.3925    | \n",
      "31      | 0.7717    | 0.3938    | \n",
      "32      | 0.7642    | 0.3922    | \n",
      "33      | 0.7786    | 0.3946    | \n",
      "34      | 0.784     | 0.4047    | \n",
      "35      | 0.7731    | 0.4067    | \n",
      "36      | 0.7809    | 0.4105    | \n",
      "37      | 0.8114    | 0.4052    | \n",
      "38      | 0.8183    | 0.4036    | \n",
      "39      | 0.817     | 0.4056    | \n",
      "40      | 0.8157    | 0.4095    | \n",
      "41      | 0.8245    | 0.4087    | \n",
      "42      | 0.8247    | 0.4107    | \n",
      "43      | 0.8333    | 0.4147    | \n",
      "44      | 0.8343    | 0.415     | \n",
      "45      | 0.8286    | 0.4205    | \n",
      "46      | 0.835     | 0.4165    | \n",
      "47      | 0.8413    | 0.4174    | \n",
      "48      | 0.8413    | 0.4142    | \n",
      "49      | 0.8464    | 0.4122    | \n",
      "50      | 0.8493    | 0.4055    | \n",
      "51      | 0.8514    | 0.407     | \n",
      "52      | 0.8561    | 0.4045    | \n",
      "53      | 0.8561    | 0.4091    | \n",
      "54      | 0.8565    | 0.4134    | \n",
      "55      | 0.8565    | 0.404     | \n",
      "56      | 0.8565    | 0.4051    | \n",
      "57      | 0.8565    | 0.4131    | \n",
      "58      | 0.8565    | 0.4138    | \n",
      "59      | 0.854     | 0.4131    | \n",
      "60      | 0.8565    | 0.411     | \n",
      "61      | 0.8565    | 0.4162    | \n",
      "62      | 0.8523    | 0.4121    | \n",
      "63      | 0.8565    | 0.4126    | \n",
      "64      | 0.8629    | 0.418     | \n",
      "65      | 0.8719    | 0.4192    | \n",
      "66      | 0.8849    | 0.4194    | \n",
      "67      | 0.8871    | 0.4201    | \n",
      "68      | 0.8841    | 0.4232    | \n",
      "69      | 0.8885    | 0.4303    | \n",
      "70      | 0.8955    | 0.4258    | \n",
      "71      | 0.8993    | 0.4261    | \n",
      "72      | 0.8957    | 0.4298    | \n",
      "73      | 0.8957    | 0.4288    | \n",
      "74      | 0.9001    | 0.4317    | \n",
      "75      | 0.9003    | 0.4338    | \n",
      "76      | 0.9005    | 0.4367    | \n",
      "77      | 0.9009    | 0.435     | \n",
      "78      | 0.8946    | 0.4365    | \n",
      "79      | 0.9073    | 0.4359    | \n",
      "80      | 0.9071    | 0.4437    | \n",
      "81      | 0.9073    | 0.4435    | \n",
      "82      | 0.9073    | 0.4464    | \n",
      "83      | 0.9073    | 0.4428    | \n",
      "84      | 0.9206    | 0.4431    | \n",
      "85      | 0.9216    | 0.4418    | \n",
      "86      | 0.9217    | 0.4387    | \n",
      "87      | 0.9281    | 0.4411    | \n",
      "88      | 0.9286    | 0.4393    | \n",
      "89      | 0.9286    | 0.4421    | \n",
      "90      | 0.9286    | 0.4475    | \n",
      "91      | 0.9408    | 0.4458    | \n",
      "92      | 0.9411    | 0.4461    | \n",
      "93      | 0.9411    | 0.444     | \n",
      "94      | 0.9432    | 0.444     | \n",
      "95      | 0.9369    | 0.4438    | \n",
      "96      | 0.9432    | 0.4438    | \n",
      "97      | 0.9432    | 0.4404    | \n",
      "98      | 0.9432    | 0.4421    | \n",
      "99      | 0.9432    | 0.442     | \n",
      "100     | 0.9436    | 0.4422    | \n",
      "101     | 0.9436    | 0.4411    | \n",
      "102     | 0.9436    | 0.4417    | \n",
      "103     | 0.9436    | 0.441     | \n",
      "104     | 0.9434    | 0.4426    | \n",
      "105     | 0.9436    | 0.44      | \n",
      "106     | 0.9436    | 0.4402    | \n",
      "107     | 0.9436    | 0.4424    | \n",
      "108     | 0.9436    | 0.4455    | \n",
      "109     | 0.9436    | 0.4437    | \n",
      "110     | 0.9457    | 0.4439    | \n",
      "111     | 0.947     | 0.4443    | \n",
      "112     | 0.947     | 0.4472    | \n",
      "113     | 0.9446    | 0.4485    | \n",
      "114     | 0.947     | 0.4474    | \n",
      "115     | 0.947     | 0.4487    | \n",
      "116     | 0.947     | 0.4495    | \n",
      "117     | 0.947     | 0.4489    | \n",
      "118     | 0.9468    | 0.4494    | \n",
      "119     | 0.947     | 0.4498    | \n",
      "120     | 0.947     | 0.4439    | \n",
      "121     | 0.947     | 0.4436    | \n",
      "122     | 0.947     | 0.4454    | \n",
      "123     | 0.947     | 0.4447    | \n",
      "124     | 0.947     | 0.4434    | \n",
      "125     | 0.947     | 0.4441    | \n",
      "126     | 0.947     | 0.4423    | \n",
      "127     | 0.947     | 0.4394    | \n",
      "128     | 0.947     | 0.4423    | \n",
      "129     | 0.947     | 0.4409    | \n",
      "130     | 0.947     | 0.4384    | \n",
      "131     | 0.947     | 0.4378    | \n",
      "132     | 0.947     | 0.4405    | \n",
      "133     | 0.947     | 0.4389    | \n",
      "134     | 0.947     | 0.4398    | \n",
      "135     | 0.947     | 0.4445    | \n",
      "136     | 0.947     | 0.4468    | \n",
      "137     | 0.947     | 0.4455    | \n",
      "138     | 0.947     | 0.4445    | \n",
      "139     | 0.947     | 0.444     | \n",
      "140     | 0.947     | 0.4448    | \n",
      "141     | 0.9513    | 0.443     | \n",
      "142     | 0.9513    | 0.4435    | \n",
      "143     | 0.9522    | 0.4422    | \n",
      "144     | 0.9522    | 0.4423    | \n",
      "145     | 0.9522    | 0.4405    | \n",
      "146     | 0.9522    | 0.4405    | \n",
      "147     | 0.9522    | 0.4389    | \n",
      "148     | 0.9522    | 0.4388    | \n",
      "149     | 0.9522    | 0.4414    | \n",
      "150     | 0.9522    | 0.4409    | \n",
      "151     | 0.9522    | 0.4432    | \n",
      "152     | 0.9522    | 0.4417    | \n",
      "153     | 0.9522    | 0.4384    | \n",
      "154     | 0.9522    | 0.4386    | \n",
      "155     | 0.9522    | 0.4388    | \n",
      "156     | 0.9522    | 0.4393    | \n",
      "157     | 0.9522    | 0.4414    | \n",
      "158     | 0.9564    | 0.4448    | \n",
      "159     | 0.9564    | 0.4449    | \n",
      "160     | 0.9564    | 0.4425    | \n",
      "161     | 0.9564    | 0.4413    | \n",
      "162     | 0.9568    | 0.4372    | \n",
      "163     | 0.9568    | 0.4375    | \n",
      "164     | 0.9568    | 0.4374    | \n",
      "165     | 0.9568    | 0.4417    | \n",
      "166     | 0.9568    | 0.4418    | \n",
      "167     | 0.9568    | 0.4405    | \n",
      "168     | 0.9568    | 0.44      | \n",
      "169     | 0.9568    | 0.44      | \n",
      "170     | 0.9568    | 0.439     | \n",
      "171     | 0.9568    | 0.4372    | \n",
      "172     | 0.9636    | 0.4383    | \n",
      "173     | 0.9636    | 0.4391    | \n",
      "174     | 0.9636    | 0.4416    | \n",
      "175     | 0.9636    | 0.4406    | \n",
      "176     | 0.9685    | 0.4432    | \n",
      "177     | 0.9685    | 0.4394    | \n",
      "178     | 0.9685    | 0.4379    | \n",
      "179     | 0.9685    | 0.4392    | \n",
      "180     | 0.9685    | 0.4391    | \n",
      "181     | 0.9685    | 0.4395    | \n",
      "182     | 0.9685    | 0.4369    | \n",
      "183     | 0.9685    | 0.4392    | \n",
      "184     | 0.9685    | 0.4417    | \n",
      "185     | 0.9685    | 0.4409    | \n",
      "186     | 0.9685    | 0.4433    | \n",
      "187     | 0.9685    | 0.4377    | \n",
      "188     | 0.9685    | 0.439     | \n",
      "189     | 0.9685    | 0.4363    | \n",
      "190     | 0.9685    | 0.4382    | \n",
      "191     | 0.9685    | 0.4338    | \n",
      "192     | 0.9685    | 0.4361    | \n",
      "193     | 0.9728    | 0.4409    | \n",
      "194     | 0.9728    | 0.4411    | \n",
      "195     | 0.9728    | 0.4387    | \n",
      "196     | 0.9772    | 0.4399    | \n",
      "197     | 0.9774    | 0.4391    | \n",
      "198     | 0.9774    | 0.4349    | \n",
      "199     | 0.9774    | 0.4369    | \n",
      "200     | 0.9774    | 0.4369    | \n",
      "201     | 0.9774    | 0.4384    | \n",
      "202     | 0.9774    | 0.4407    | \n",
      "203     | 0.9774    | 0.4405    | \n",
      "204     | 0.9774    | 0.4381    | \n",
      "205     | 0.9778    | 0.4378    | \n",
      "206     | 0.9778    | 0.435     | \n",
      "207     | 0.9778    | 0.4356    | \n",
      "208     | 0.9778    | 0.4356    | \n",
      "209     | 0.9778    | 0.4356    | \n",
      "210     | 0.9778    | 0.4333    | \n",
      "211     | 0.9778    | 0.4377    | \n",
      "212     | 0.9778    | 0.4361    | \n",
      "213     | 0.9778    | 0.4371    | \n",
      "214     | 0.9778    | 0.434     | \n",
      "215     | 0.9778    | 0.4348    | \n",
      "216     | 0.9778    | 0.4288    | \n",
      "217     | 0.9778    | 0.4306    | \n",
      "218     | 0.9778    | 0.4328    | \n",
      "219     | 0.9778    | 0.4316    | \n",
      "220     | 0.9778    | 0.4308    | \n",
      "---------------------------------\n",
      "Finished sucessfully.\n",
      "NDCG@10 on training data: 0.947\n",
      "NDCG@10 on validation data: 0.4498\n",
      "---------------------------------\n",
      "ERR@10 on test data: 0.8911\n",
      "\n",
      "Model saved to: ../Data/trained_models_2020_2021/sbert_models/sbert_model_2021__LambdaMart.txt\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -train ../Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank -test ../Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank -tvs .2 -ranker 6 -metric2t NDCG@10 -metric2T ERR@10 -save ../Data/trained_models_2020_2021/sbert_models/sbert_model_2021__LambdaMart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Training data:\t../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank\n",
      "Test data:\t../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank\n",
      "Train-Validation split: 0.2\n",
      "Feature vector representation: Dense.\n",
      "Ranking method:\tLambdaMART\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@10\n",
      "Test metric:\tERR@10\n",
      "Highest relevance label (to compute ERR): 4\n",
      "Feature normalization: No\n",
      "Model file: ../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2021__LambdaMart.txt\n",
      "\n",
      "[+] LambdaMART's Parameters:\n",
      "No. of trees: 1000\n",
      "No. of leaves: 10\n",
      "No. of threshold candidates: 256\n",
      "Min leaf support: 1\n",
      "Learning rate: 0.1\n",
      "Stop early: 100 rounds without performance gain on validation data\n",
      "\n",
      "Reading feature file [../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank]... [Done.]            \n",
      "(50 ranked lists, 3442 entries read)\n",
      "Reading feature file [../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank]... [Done.]            \n",
      "(49 ranked lists, 1806 entries read)\n",
      "Initializing... [Done]\n",
      "---------------------------------\n",
      "Training starts...\n",
      "---------------------------------\n",
      "#iter   | NDCG@10-T | NDCG@10-V | \n",
      "---------------------------------\n",
      "1       | 0.4866    | 0.3857    | \n",
      "2       | 0.6344    | 0.4087    | \n",
      "3       | 0.664     | 0.4228    | \n",
      "4       | 0.6909    | 0.4103    | \n",
      "5       | 0.7267    | 0.4209    | \n",
      "6       | 0.724     | 0.4242    | \n",
      "7       | 0.7297    | 0.4306    | \n",
      "8       | 0.7412    | 0.4262    | \n",
      "9       | 0.7416    | 0.4267    | \n",
      "10      | 0.7341    | 0.422     | \n",
      "11      | 0.7749    | 0.432     | \n",
      "12      | 0.7806    | 0.4415    | \n",
      "13      | 0.7707    | 0.4518    | \n",
      "14      | 0.788     | 0.4482    | \n",
      "15      | 0.7827    | 0.444     | \n",
      "16      | 0.784     | 0.4514    | \n",
      "17      | 0.799     | 0.4626    | \n",
      "18      | 0.8018    | 0.4624    | \n",
      "19      | 0.7938    | 0.4557    | \n",
      "20      | 0.7932    | 0.454     | \n",
      "21      | 0.7941    | 0.4548    | \n",
      "22      | 0.7954    | 0.4564    | \n",
      "23      | 0.7907    | 0.4557    | \n",
      "24      | 0.794     | 0.4558    | \n",
      "25      | 0.7881    | 0.455     | \n",
      "26      | 0.7968    | 0.4536    | \n",
      "27      | 0.7972    | 0.4516    | \n",
      "28      | 0.7972    | 0.4568    | \n",
      "29      | 0.7992    | 0.455     | \n",
      "30      | 0.8058    | 0.4555    | \n",
      "31      | 0.7926    | 0.4574    | \n",
      "32      | 0.7975    | 0.4571    | \n",
      "33      | 0.8156    | 0.457     | \n",
      "34      | 0.8146    | 0.4572    | \n",
      "35      | 0.8177    | 0.4578    | \n",
      "36      | 0.8296    | 0.456     | \n",
      "37      | 0.8258    | 0.459     | \n",
      "38      | 0.8349    | 0.4633    | \n",
      "39      | 0.8333    | 0.4637    | \n",
      "40      | 0.8388    | 0.4608    | \n",
      "41      | 0.8321    | 0.4598    | \n",
      "42      | 0.8402    | 0.4592    | \n",
      "43      | 0.8416    | 0.4647    | \n",
      "44      | 0.845     | 0.4657    | \n",
      "45      | 0.8516    | 0.4676    | \n",
      "46      | 0.8482    | 0.4683    | \n",
      "47      | 0.8506    | 0.4677    | \n",
      "48      | 0.8613    | 0.4626    | \n",
      "49      | 0.8549    | 0.4633    | \n",
      "50      | 0.8555    | 0.4685    | \n",
      "51      | 0.8652    | 0.4685    | \n",
      "52      | 0.8677    | 0.4665    | \n",
      "53      | 0.8739    | 0.4702    | \n",
      "54      | 0.869     | 0.4712    | \n",
      "55      | 0.8714    | 0.4739    | \n",
      "56      | 0.8742    | 0.4759    | \n",
      "57      | 0.8735    | 0.4753    | \n",
      "58      | 0.8799    | 0.4755    | \n",
      "59      | 0.8799    | 0.4729    | \n",
      "60      | 0.8859    | 0.4755    | \n",
      "61      | 0.8934    | 0.4793    | \n",
      "62      | 0.8932    | 0.4772    | \n",
      "63      | 0.8934    | 0.481     | \n",
      "64      | 0.8865    | 0.4794    | \n",
      "65      | 0.8961    | 0.4864    | \n",
      "66      | 0.893     | 0.4878    | \n",
      "67      | 0.8951    | 0.4825    | \n",
      "68      | 0.8957    | 0.4794    | \n",
      "69      | 0.9068    | 0.4849    | \n",
      "70      | 0.9064    | 0.4806    | \n",
      "71      | 0.9086    | 0.4833    | \n",
      "72      | 0.9068    | 0.4802    | \n",
      "73      | 0.9098    | 0.4793    | \n",
      "74      | 0.9145    | 0.4791    | \n",
      "75      | 0.9063    | 0.4772    | \n",
      "76      | 0.9063    | 0.4794    | \n",
      "77      | 0.9125    | 0.4787    | \n",
      "78      | 0.9169    | 0.4811    | \n",
      "79      | 0.9169    | 0.4801    | \n",
      "80      | 0.9122    | 0.4809    | \n",
      "81      | 0.9158    | 0.4858    | \n",
      "82      | 0.9114    | 0.4858    | \n",
      "83      | 0.9181    | 0.4862    | \n",
      "84      | 0.9186    | 0.4828    | \n",
      "85      | 0.9202    | 0.4847    | \n",
      "86      | 0.9133    | 0.4857    | \n",
      "87      | 0.9167    | 0.4826    | \n",
      "88      | 0.9191    | 0.4851    | \n",
      "89      | 0.9164    | 0.4857    | \n",
      "90      | 0.9177    | 0.4862    | \n",
      "91      | 0.9172    | 0.4842    | \n",
      "92      | 0.9239    | 0.4854    | \n",
      "93      | 0.9245    | 0.4901    | \n",
      "94      | 0.9207    | 0.4916    | \n",
      "95      | 0.9192    | 0.491     | \n",
      "96      | 0.913     | 0.4895    | \n",
      "97      | 0.9154    | 0.4901    | \n",
      "98      | 0.9178    | 0.4902    | \n",
      "99      | 0.918     | 0.4882    | \n",
      "100     | 0.9174    | 0.4921    | \n",
      "101     | 0.926     | 0.4929    | \n",
      "102     | 0.9255    | 0.4906    | \n",
      "103     | 0.9265    | 0.4903    | \n",
      "104     | 0.9227    | 0.4901    | \n",
      "105     | 0.9264    | 0.4914    | \n",
      "106     | 0.9263    | 0.4962    | \n",
      "107     | 0.9247    | 0.4932    | \n",
      "108     | 0.9261    | 0.4931    | \n",
      "109     | 0.9265    | 0.4953    | \n",
      "110     | 0.925     | 0.4952    | \n",
      "111     | 0.9275    | 0.4937    | \n",
      "112     | 0.9275    | 0.4935    | \n",
      "113     | 0.9282    | 0.4933    | \n",
      "114     | 0.9213    | 0.4938    | \n",
      "115     | 0.9217    | 0.4914    | \n",
      "116     | 0.9218    | 0.4909    | \n",
      "117     | 0.924     | 0.4913    | \n",
      "118     | 0.9254    | 0.4911    | \n",
      "119     | 0.9192    | 0.4909    | \n",
      "120     | 0.9258    | 0.4914    | \n",
      "121     | 0.9236    | 0.4925    | \n",
      "122     | 0.9277    | 0.498     | \n",
      "123     | 0.9232    | 0.4935    | \n",
      "124     | 0.9217    | 0.4988    | \n",
      "125     | 0.9248    | 0.4978    | \n",
      "126     | 0.925     | 0.4983    | \n",
      "127     | 0.9234    | 0.4961    | \n",
      "128     | 0.9245    | 0.4976    | \n",
      "129     | 0.9229    | 0.4984    | \n",
      "130     | 0.9272    | 0.4987    | \n",
      "131     | 0.9203    | 0.4971    | \n",
      "132     | 0.9213    | 0.4973    | \n",
      "133     | 0.9185    | 0.4957    | \n",
      "134     | 0.9185    | 0.4949    | \n",
      "135     | 0.927     | 0.4975    | \n",
      "136     | 0.926     | 0.4986    | \n",
      "137     | 0.927     | 0.4944    | \n",
      "138     | 0.927     | 0.4946    | \n",
      "139     | 0.927     | 0.4949    | \n",
      "140     | 0.927     | 0.495     | \n",
      "141     | 0.927     | 0.4969    | \n",
      "142     | 0.9272    | 0.4959    | \n",
      "143     | 0.9321    | 0.4962    | \n",
      "144     | 0.9318    | 0.4996    | \n",
      "145     | 0.9277    | 0.4978    | \n",
      "146     | 0.932     | 0.4971    | \n",
      "147     | 0.932     | 0.4979    | \n",
      "148     | 0.932     | 0.4981    | \n",
      "149     | 0.9256    | 0.4948    | \n",
      "150     | 0.9254    | 0.4947    | \n",
      "151     | 0.923     | 0.4952    | \n",
      "152     | 0.923     | 0.4931    | \n",
      "153     | 0.9296    | 0.4934    | \n",
      "154     | 0.9254    | 0.4933    | \n",
      "155     | 0.9205    | 0.4949    | \n",
      "156     | 0.9166    | 0.4942    | \n",
      "157     | 0.9254    | 0.4953    | \n",
      "158     | 0.9296    | 0.4948    | \n",
      "159     | 0.9296    | 0.4945    | \n",
      "160     | 0.9296    | 0.495     | \n",
      "161     | 0.9272    | 0.495     | \n",
      "162     | 0.9296    | 0.495     | \n",
      "163     | 0.9271    | 0.4957    | \n",
      "164     | 0.9296    | 0.4948    | \n",
      "165     | 0.9345    | 0.4944    | \n",
      "166     | 0.9349    | 0.4917    | \n",
      "167     | 0.9349    | 0.4911    | \n",
      "168     | 0.9391    | 0.4897    | \n",
      "169     | 0.9447    | 0.4881    | \n",
      "170     | 0.9447    | 0.488     | \n",
      "171     | 0.9447    | 0.4879    | \n",
      "172     | 0.9447    | 0.4886    | \n",
      "173     | 0.946     | 0.4909    | \n",
      "174     | 0.946     | 0.4902    | \n",
      "175     | 0.9417    | 0.4882    | \n",
      "176     | 0.9459    | 0.4916    | \n",
      "177     | 0.9459    | 0.4914    | \n",
      "178     | 0.9466    | 0.4895    | \n",
      "179     | 0.9468    | 0.4881    | \n",
      "180     | 0.9472    | 0.4893    | \n",
      "181     | 0.9472    | 0.4895    | \n",
      "182     | 0.9472    | 0.4883    | \n",
      "183     | 0.9515    | 0.4875    | \n",
      "184     | 0.9517    | 0.4892    | \n",
      "185     | 0.9517    | 0.4876    | \n",
      "186     | 0.9517    | 0.4894    | \n",
      "187     | 0.9517    | 0.4905    | \n",
      "188     | 0.9517    | 0.4924    | \n",
      "189     | 0.9517    | 0.4949    | \n",
      "190     | 0.9475    | 0.494     | \n",
      "191     | 0.9494    | 0.4927    | \n",
      "192     | 0.9494    | 0.4927    | \n",
      "193     | 0.9517    | 0.4943    | \n",
      "194     | 0.9517    | 0.4956    | \n",
      "195     | 0.9537    | 0.4956    | \n",
      "196     | 0.9517    | 0.5004    | \n",
      "197     | 0.9517    | 0.5013    | \n",
      "198     | 0.9532    | 0.5022    | \n",
      "199     | 0.9512    | 0.4992    | \n",
      "200     | 0.9512    | 0.4989    | \n",
      "201     | 0.9512    | 0.4987    | \n",
      "202     | 0.9532    | 0.499     | \n",
      "203     | 0.9512    | 0.4972    | \n",
      "204     | 0.9532    | 0.4961    | \n",
      "205     | 0.9512    | 0.4974    | \n",
      "206     | 0.9582    | 0.5001    | \n",
      "207     | 0.9585    | 0.5       | \n",
      "208     | 0.9585    | 0.5003    | \n",
      "209     | 0.9585    | 0.5016    | \n",
      "210     | 0.9604    | 0.5016    | \n",
      "211     | 0.9588    | 0.5003    | \n",
      "212     | 0.9588    | 0.4987    | \n",
      "213     | 0.9588    | 0.5017    | \n",
      "214     | 0.9586    | 0.5025    | \n",
      "215     | 0.9588    | 0.5045    | \n",
      "216     | 0.9608    | 0.5042    | \n",
      "217     | 0.9588    | 0.5049    | \n",
      "218     | 0.9608    | 0.505     | \n",
      "219     | 0.9595    | 0.5056    | \n",
      "220     | 0.9688    | 0.5065    | \n",
      "221     | 0.9669    | 0.5071    | \n",
      "222     | 0.9646    | 0.5111    | \n",
      "223     | 0.9646    | 0.5151    | \n",
      "224     | 0.9649    | 0.5148    | \n",
      "225     | 0.9621    | 0.5163    | \n",
      "226     | 0.9649    | 0.5155    | \n",
      "227     | 0.9649    | 0.5161    | \n",
      "228     | 0.9649    | 0.5176    | \n",
      "229     | 0.9585    | 0.5188    | \n",
      "230     | 0.9649    | 0.5219    | \n",
      "231     | 0.9649    | 0.5202    | \n",
      "232     | 0.9652    | 0.5192    | \n",
      "233     | 0.9695    | 0.521     | \n",
      "234     | 0.968     | 0.5221    | \n",
      "235     | 0.968     | 0.5232    | \n",
      "236     | 0.968     | 0.5241    | \n",
      "237     | 0.9614    | 0.5244    | \n",
      "238     | 0.97      | 0.5247    | \n",
      "239     | 0.968     | 0.5254    | \n",
      "240     | 0.968     | 0.5268    | \n",
      "241     | 0.97      | 0.5266    | \n",
      "242     | 0.968     | 0.5263    | \n",
      "243     | 0.97      | 0.5267    | \n",
      "244     | 0.968     | 0.5267    | \n",
      "245     | 0.968     | 0.5265    | \n",
      "246     | 0.968     | 0.5241    | \n",
      "247     | 0.97      | 0.5232    | \n",
      "248     | 0.968     | 0.5234    | \n",
      "249     | 0.97      | 0.5235    | \n",
      "250     | 0.968     | 0.5235    | \n",
      "251     | 0.97      | 0.5244    | \n",
      "252     | 0.9684    | 0.5246    | \n",
      "253     | 0.9703    | 0.5225    | \n",
      "254     | 0.9684    | 0.5251    | \n",
      "255     | 0.9684    | 0.5248    | \n",
      "256     | 0.9703    | 0.5242    | \n",
      "257     | 0.9684    | 0.5247    | \n",
      "258     | 0.9703    | 0.5222    | \n",
      "259     | 0.9703    | 0.5237    | \n",
      "260     | 0.9684    | 0.5247    | \n",
      "261     | 0.9703    | 0.5223    | \n",
      "262     | 0.9684    | 0.5178    | \n",
      "263     | 0.9684    | 0.5171    | \n",
      "264     | 0.9618    | 0.5185    | \n",
      "265     | 0.9703    | 0.5184    | \n",
      "266     | 0.9684    | 0.5191    | \n",
      "267     | 0.9684    | 0.5194    | \n",
      "268     | 0.9703    | 0.5191    | \n",
      "269     | 0.9684    | 0.5194    | \n",
      "270     | 0.9684    | 0.5194    | \n",
      "271     | 0.9684    | 0.5193    | \n",
      "272     | 0.9705    | 0.5188    | \n",
      "273     | 0.9684    | 0.518     | \n",
      "274     | 0.9705    | 0.5181    | \n",
      "275     | 0.9705    | 0.5182    | \n",
      "276     | 0.9705    | 0.5194    | \n",
      "277     | 0.9684    | 0.5194    | \n",
      "278     | 0.9705    | 0.5189    | \n",
      "279     | 0.9705    | 0.5187    | \n",
      "280     | 0.9705    | 0.5209    | \n",
      "281     | 0.9705    | 0.5207    | \n",
      "282     | 0.9684    | 0.5199    | \n",
      "283     | 0.9705    | 0.5199    | \n",
      "284     | 0.9705    | 0.5225    | \n",
      "285     | 0.9684    | 0.5248    | \n",
      "286     | 0.9705    | 0.5218    | \n",
      "287     | 0.9705    | 0.5246    | \n",
      "288     | 0.9705    | 0.5239    | \n",
      "289     | 0.9714    | 0.5251    | \n",
      "290     | 0.9735    | 0.524     | \n",
      "291     | 0.9735    | 0.5248    | \n",
      "292     | 0.9714    | 0.5253    | \n",
      "293     | 0.9735    | 0.5217    | \n",
      "294     | 0.9711    | 0.5236    | \n",
      "295     | 0.9735    | 0.5201    | \n",
      "296     | 0.9714    | 0.5209    | \n",
      "297     | 0.9735    | 0.5172    | \n",
      "298     | 0.9714    | 0.5193    | \n",
      "299     | 0.9735    | 0.517     | \n",
      "300     | 0.9714    | 0.5188    | \n",
      "301     | 0.9714    | 0.5188    | \n",
      "302     | 0.9735    | 0.5167    | \n",
      "303     | 0.9714    | 0.5167    | \n",
      "304     | 0.9735    | 0.5182    | \n",
      "305     | 0.9765    | 0.518     | \n",
      "306     | 0.9786    | 0.5198    | \n",
      "307     | 0.979     | 0.521     | \n",
      "308     | 0.9774    | 0.5226    | \n",
      "309     | 0.9797    | 0.5211    | \n",
      "310     | 0.9797    | 0.5232    | \n",
      "311     | 0.9797    | 0.5212    | \n",
      "312     | 0.9839    | 0.5228    | \n",
      "313     | 0.9839    | 0.5215    | \n",
      "314     | 0.9839    | 0.5237    | \n",
      "315     | 0.9839    | 0.5238    | \n",
      "316     | 0.9839    | 0.5248    | \n",
      "317     | 0.9838    | 0.5255    | \n",
      "318     | 0.9838    | 0.5243    | \n",
      "319     | 0.9838    | 0.523     | \n",
      "320     | 0.9838    | 0.5235    | \n",
      "321     | 0.9838    | 0.5238    | \n",
      "322     | 0.9838    | 0.5238    | \n",
      "323     | 0.9838    | 0.5249    | \n",
      "324     | 0.9838    | 0.5248    | \n",
      "325     | 0.9838    | 0.5266    | \n",
      "326     | 0.9838    | 0.5287    | \n",
      "327     | 0.9838    | 0.5253    | \n",
      "328     | 0.9838    | 0.5236    | \n",
      "329     | 0.9838    | 0.5219    | \n",
      "330     | 0.9838    | 0.524     | \n",
      "331     | 0.9838    | 0.5223    | \n",
      "332     | 0.9838    | 0.5225    | \n",
      "333     | 0.9838    | 0.5172    | \n",
      "334     | 0.9838    | 0.5175    | \n",
      "335     | 0.9838    | 0.5198    | \n",
      "336     | 0.9838    | 0.52      | \n",
      "337     | 0.9838    | 0.52      | \n",
      "338     | 0.9838    | 0.5189    | \n",
      "339     | 0.9841    | 0.5195    | \n",
      "340     | 0.9841    | 0.5182    | \n",
      "341     | 0.9841    | 0.5197    | \n",
      "342     | 0.9842    | 0.5197    | \n",
      "343     | 0.9842    | 0.5203    | \n",
      "344     | 0.9842    | 0.518     | \n",
      "345     | 0.9842    | 0.5165    | \n",
      "346     | 0.9842    | 0.5174    | \n",
      "347     | 0.9842    | 0.5169    | \n",
      "348     | 0.9842    | 0.5175    | \n",
      "349     | 0.9842    | 0.5172    | \n",
      "350     | 0.9842    | 0.5156    | \n",
      "351     | 0.9842    | 0.5172    | \n",
      "352     | 0.9842    | 0.5182    | \n",
      "353     | 0.9842    | 0.5177    | \n",
      "354     | 0.9842    | 0.5186    | \n",
      "355     | 0.9842    | 0.5186    | \n",
      "356     | 0.9842    | 0.5208    | \n",
      "357     | 0.9842    | 0.5209    | \n",
      "358     | 0.9842    | 0.518     | \n",
      "359     | 0.9842    | 0.5191    | \n",
      "360     | 0.9842    | 0.5191    | \n",
      "361     | 0.9842    | 0.5225    | \n",
      "362     | 0.9818    | 0.5222    | \n",
      "363     | 0.9842    | 0.5187    | \n",
      "364     | 0.9842    | 0.519     | \n",
      "365     | 0.9842    | 0.5201    | \n",
      "366     | 0.9842    | 0.5219    | \n",
      "367     | 0.9842    | 0.5227    | \n",
      "368     | 0.9844    | 0.5237    | \n",
      "369     | 0.9844    | 0.5237    | \n",
      "370     | 0.9844    | 0.5229    | \n",
      "371     | 0.9844    | 0.5227    | \n",
      "372     | 0.9844    | 0.5223    | \n",
      "373     | 0.9844    | 0.5204    | \n",
      "374     | 0.9844    | 0.5211    | \n",
      "375     | 0.9844    | 0.5212    | \n",
      "376     | 0.9844    | 0.5215    | \n",
      "377     | 0.9844    | 0.5215    | \n",
      "378     | 0.9844    | 0.5216    | \n",
      "379     | 0.9913    | 0.5208    | \n",
      "380     | 0.9913    | 0.5202    | \n",
      "381     | 0.9913    | 0.5227    | \n",
      "382     | 0.9913    | 0.523     | \n",
      "383     | 0.9913    | 0.5226    | \n",
      "384     | 0.9913    | 0.522     | \n",
      "385     | 0.9913    | 0.5228    | \n",
      "386     | 0.9913    | 0.5198    | \n",
      "387     | 0.9913    | 0.5186    | \n",
      "388     | 0.9913    | 0.5225    | \n",
      "389     | 0.9913    | 0.5207    | \n",
      "390     | 0.9913    | 0.5191    | \n",
      "391     | 0.9913    | 0.5186    | \n",
      "392     | 0.9913    | 0.5182    | \n",
      "393     | 0.9913    | 0.5205    | \n",
      "394     | 0.9913    | 0.5181    | \n",
      "395     | 0.9913    | 0.5181    | \n",
      "396     | 0.9913    | 0.5174    | \n",
      "397     | 0.9913    | 0.5171    | \n",
      "398     | 0.9913    | 0.5157    | \n",
      "399     | 0.9913    | 0.517     | \n",
      "400     | 0.9913    | 0.5166    | \n",
      "401     | 0.9913    | 0.5193    | \n",
      "402     | 0.9913    | 0.5175    | \n",
      "403     | 0.9913    | 0.5192    | \n",
      "404     | 0.9913    | 0.5214    | \n",
      "405     | 0.9913    | 0.5228    | \n",
      "406     | 0.9913    | 0.5227    | \n",
      "407     | 0.9913    | 0.5247    | \n",
      "408     | 0.9913    | 0.5233    | \n",
      "409     | 0.9913    | 0.5232    | \n",
      "410     | 0.9912    | 0.5233    | \n",
      "411     | 0.9912    | 0.5238    | \n",
      "412     | 0.9912    | 0.5247    | \n",
      "413     | 0.9912    | 0.5259    | \n",
      "414     | 0.9912    | 0.5247    | \n",
      "415     | 0.9915    | 0.5236    | \n",
      "416     | 0.9915    | 0.5235    | \n",
      "417     | 0.9915    | 0.5235    | \n",
      "418     | 0.9915    | 0.5237    | \n",
      "419     | 0.9915    | 0.5234    | \n",
      "420     | 0.9915    | 0.525     | \n",
      "421     | 0.9915    | 0.5243    | \n",
      "422     | 0.9915    | 0.519     | \n",
      "423     | 0.9915    | 0.5173    | \n",
      "424     | 0.9915    | 0.5174    | \n",
      "425     | 0.9915    | 0.5173    | \n",
      "426     | 0.9915    | 0.5174    | \n",
      "427     | 0.9915    | 0.5181    | \n",
      "---------------------------------\n",
      "Finished sucessfully.\n",
      "NDCG@10 on training data: 0.9838\n",
      "NDCG@10 on validation data: 0.5287\n",
      "---------------------------------\n",
      "ERR@10 on test data: 0.8261\n",
      "\n",
      "Model saved to: ../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2021__LambdaMart.txt\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -train ../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank -test ../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank -tvs .2 -ranker 6 -metric2t NDCG@10 -metric2T ERR@10 -save ../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2021__LambdaMart.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank 2020 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Model file:\t../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2021.txt\n",
      "Feature normalization: No\n",
      "Model:\t\tRandom Forests\n",
      "Reading feature file [../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2020.svmrank]... [Done.]            \n",
      "(49 ranked lists, 1806 entries read)\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -load ../Data/trained_models_2020_2021/linguistic_sentiment_models/linguistic_sentiment__model_2021.txt -rank ../Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2020.svmrank -score ../Data/scorefiles_2020_2021/linguistic_sentiment_scorefiles/argument2020_scorefile_LambdaMart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Model file:\t../Data/trained_models_2020_2021/sbert_models/sbert_model_2021.txt\n",
      "Feature normalization: No\n",
      "Model:\t\tRandom Forests\n",
      "Reading feature file [../Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank]... [Done.]            \n",
      "(49 ranked lists, 3612 entries read)\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -load ../Data/trained_models_2020_2021/sbert_models/sbert_model_2021.txt -rank ../Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank -score ../Data/scorefiles_2020_2021/sbert_scorefiles/argument2020_scorefile_LambdaMart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Model file:\t../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2021.txt\n",
      "Feature normalization: No\n",
      "Model:\t\tRandom Forests\n",
      "Reading feature file [../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank]... [Done.]            \n",
      "(49 ranked lists, 1806 entries read)\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../ranking-model/RankLib-2.18.jar -load ../Data/trained_models_2020_2021/sentiment_sarcasm_models/sentiment_sarcasm_model_2021.txt -rank ../Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank -score ../Data/scorefiles_2020_2021/sentiment_sarcasm_scorefiles/argument2020_scorefile_LambdaMart.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

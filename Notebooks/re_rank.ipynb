{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1: `Adding scores to ranking files`\n",
    "\n",
    "It takes in three file paths as input arguments: `ranking_file`, `scores_file`, and `output_file`. \n",
    "\n",
    "- `ranking_file` is a text file containing a list of documents, where each document is followed by a rank number and a \"#\" symbol. The function extracts the document IDs from each line and saves them for later use.\n",
    "- `scores_file` is a text file containing a list of scores, where each score corresponds to the document at the same line number in `ranking_file`. The function reads in these scores and stores them in memory.\n",
    "- The function then writes out a new file at `output_file`, where each line corresponds to a document in `ranking_file`, with the score from `scores_file` appended to the end of the line, preceded by a \"#\" symbol and the document ID.\n",
    "\n",
    "### Function 2: `Re-ranking documents`\n",
    "\n",
    "It takes in three file paths as input arguments: `argument_scorefile_with_doc_ids`, `merged_data_no_relevance`, and `output_file`.\n",
    "\n",
    "- `argument_scorefile_with_doc_ids` is a text file containing a list of scores and their corresponding document IDs, separated by \"#\" symbols. The function reads in these scores and document IDs and stores them in memory.\n",
    "- `merged_data_no_relevance` is a text file containing a list of data points in SVMrank format, where each line corresponds to a document and contains a query ID, a document rank, and a feature vector. The function reads in this data and modifies it to include the scores from `argument_scorefile_with_doc_ids`, appending them to the feature vector of each document.\n",
    "- The function then reorders the data based on the query ID and the new scores (from high to low), and writes out a new file at `output_file` with the reordered data.\n",
    "\n",
    "If there are no document IDs that match between `argument_scorefile_with_doc_ids` and `merged_data_no_relevance`, the function will print a message indicating that no matching IDs were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "data_path = '/Users/balazs/Desktop/dissertationProjectCode/dissertationCodeBase/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_argument_ids_to_scorefile(ranking_file, scores_file, output_file):\n",
    "    with open(ranking_file, 'r') as rankfile, open(scores_file, 'r') as scorefile, open(output_file, 'w') as outfile:\n",
    "        for line, score in zip(rankfile, scorefile):\n",
    "            doc_id = line.split('#')[-1].strip()\n",
    "            outfile.write(f'{score.strip()} #{doc_id}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_rank_svmrank_data(argument_scorefile_with_doc_ids, merged_data_no_relevance, output_file):\n",
    "    # Read the argument2020_scorefile_with_doc_ids file and store the scores in a dictionary\n",
    "    with open(argument_scorefile_with_doc_ids, \"r\") as f:\n",
    "        scores = {}\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"#\")\n",
    "            doc_id = parts[1].strip()\n",
    "            score = float(parts[0].split()[2])\n",
    "            scores[doc_id] = score\n",
    "\n",
    "    # Read the merged_data_no_relevance_2020.svmrank file, add the predicted scores, and store the lines in a list\n",
    "    with open(merged_data_no_relevance, \"r\") as f:\n",
    "        merged_data = []\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"#\")\n",
    "            doc_id = parts[1].strip()\n",
    "            if doc_id in scores:\n",
    "                relevance_score = str(scores[doc_id])\n",
    "                qid = parts[0].split()[0]  # Extract qid\n",
    "                features = \" \".join(parts[0].split()[1:])  # Extract features\n",
    "                new_line = relevance_score + \" \" + qid + \" \" + features + \" #\" + doc_id\n",
    "                merged_data.append((qid, scores[doc_id], new_line))\n",
    "\n",
    "    # Check if any lines were matched and modified\n",
    "    if merged_data:\n",
    "        # Reorder the merged data based on qid and the new scores (high to low)\n",
    "        merged_data.sort(key=lambda x: (x[0], -x[1]))\n",
    "\n",
    "        # Write the reordered data to the output file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for _, _, line in merged_data:\n",
    "                f.write(line + \"\\n\")\n",
    "    else:\n",
    "        print(\"No matching IDs found between the two files.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic & Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scorefile update 2021 paths\n",
    "ranking_file_2021 = data_path + 'Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2021.svmrank'\n",
    "scores_file_2021 = data_path + 'Data/scorefiles_2020_2021/linguistic_sentiment_scorefiles/argument2021_scorefile_LambdaMart.txt'\n",
    "output_file_2021 = data_path + 'Data/scorefiles_2020_2021/linguistic_sentiment_scorefiles/argument2021_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "\n",
    "# Scorefile update 2020 paths\n",
    "ranking_file_2020 = data_path + 'Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_2020.svmrank'\n",
    "scores_file_2020 = data_path + 'Data/scorefiles_2020_2021/linguistic_sentiment_scorefiles/argument2020_scorefile_LambdaMart.txt'\n",
    "output_file_2020 = data_path + 'Data/scorefiles_2020_2021/linguistic_sentiment_scorefiles/argument2020_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "\n",
    "# Add argument ID's to scorefile 2021\n",
    "add_argument_ids_to_scorefile(ranking_file_2021, scores_file_2021, output_file_2021)\n",
    "\n",
    "# Add argument ID's to scorefile 2020\n",
    "add_argument_ids_to_scorefile(ranking_file_2020, scores_file_2020, output_file_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-rank 2021 paths\n",
    "argument2021_scorefile_with_doc_ids = data_path + 'Data/scorefiles_2020_2021/linguistic_sentiment_scorefiles/argument2021_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "merged_data_no_relevance_2021 = data_path + 'Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_no_relevance_2021.svmrank'\n",
    "re_ranked_2021 = data_path + 'Data/re_ranked_2020_2021/linguistic_sentiment_re_ranked/re_ranked_2021_LambdaMart.txt'\n",
    "\n",
    "#Re-rank 2020 paths\n",
    "argument2020_scorefile_with_doc_ids = data_path + 'Data/scorefiles_2020_2021/linguistic_sentiment_scorefiles/argument2020_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "merged_data_no_relevance_2020 = data_path + 'Data/svm_format_2020_2021/linguistic_sentiment_merged/merged_data_no_relevance_2020.svmrank'\n",
    "re_ranked_2020 = data_path + 'Data/re_ranked_2020_2021/linguistic_sentiment_re_ranked/re_ranked_2020_LambdaMart.txt'\n",
    "\n",
    "# Re-rank 2020\n",
    "re_rank_svmrank_data(argument2020_scorefile_with_doc_ids, merged_data_no_relevance_2020, re_ranked_2020)\n",
    "\n",
    "#Re-rank 2021\n",
    "re_rank_svmrank_data(argument2021_scorefile_with_doc_ids, merged_data_no_relevance_2021, re_ranked_2021)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scorefile update 2021 paths\n",
    "ranking_file_2021 = data_path + 'Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank'\n",
    "scores_file_2021 = data_path + 'Data/scorefiles_2020_2021/sbert_scorefiles/argument2021_scorefile_LambdaMart.txt'\n",
    "output_file_2021 = data_path + 'Data/scorefiles_2020_2021/sbert_scorefiles/argument2021_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "\n",
    "# Scorefile update 2020 paths\n",
    "ranking_file_2020 = data_path + 'Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank'\n",
    "scores_file_2020 = data_path + 'Data/scorefiles_2020_2021/sbert_scorefiles/argument2020_scorefile_LambdaMart.txt'\n",
    "output_file_2020 = data_path + 'Data/scorefiles_2020_2021/sbert_scorefiles/argument2020_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "\n",
    "# Add argument ID's to scorefile 2021\n",
    "add_argument_ids_to_scorefile(ranking_file_2021, scores_file_2021, output_file_2021)\n",
    "\n",
    "# Add argument ID's to scorefile 2020\n",
    "add_argument_ids_to_scorefile(ranking_file_2020, scores_file_2020, output_file_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-rank 2021 paths\n",
    "argument2021_scorefile_with_doc_ids = data_path + 'Data/scorefiles_2020_2021/sbert_scorefiles/argument2021_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "merged_data_no_relevance_2021 = data_path + 'Data/svm_format_2020_2021/sbert_merged/merged_data_no_relevance_2021.svmrank'\n",
    "re_ranked_2021 = data_path + 'Data/re_ranked_2020_2021/sbert_re_ranked/re_ranked_2021_LambdaMart.txt'\n",
    "\n",
    "#Re-rank 2020 paths\n",
    "argument2020_scorefile_with_doc_ids = data_path + 'Data/scorefiles_2020_2021/sbert_scorefiles/argument2020_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "merged_data_no_relevance_2020 = data_path + 'Data/svm_format_2020_2021/sbert_merged/merged_data_no_relevance_2020.svmrank'\n",
    "re_ranked_2020 = data_path + 'Data/re_ranked_2020_2021/sbert_re_ranked/re_ranked_2020_LambdaMart.txt'\n",
    "\n",
    "# Re-rank 2020\n",
    "re_rank_svmrank_data(argument2020_scorefile_with_doc_ids, merged_data_no_relevance_2020, re_ranked_2020)\n",
    "\n",
    "#Re-rank 2021\n",
    "re_rank_svmrank_data(argument2021_scorefile_with_doc_ids, merged_data_no_relevance_2021, re_ranked_2021)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic & Sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scorefile update 2021 paths\n",
    "ranking_file_2021 = data_path + 'Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank'\n",
    "scores_file_2021 = data_path + 'Data/scorefiles_2020_2021/sentiment_sarcasm_scorefiles/argument2021_scorefile_LambdaMart.txt'\n",
    "output_file_2021 = data_path + 'Data/scorefiles_2020_2021/sentiment_sarcasm_scorefiles/argument2021_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "\n",
    "# Scorefile update 2020 paths\n",
    "ranking_file_2020 = data_path + 'Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank'\n",
    "scores_file_2020 = data_path + 'Data/scorefiles_2020_2021/sentiment_sarcasm_scorefiles/argument2020_scorefile_LambdaMart.txt'\n",
    "output_file_2020 = data_path + 'Data/scorefiles_2020_2021/sentiment_sarcasm_scorefiles/argument2020_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "\n",
    "# Add argument ID's to scorefile 2021\n",
    "add_argument_ids_to_scorefile(ranking_file_2021, scores_file_2021, output_file_2021)\n",
    "\n",
    "# Add argument ID's to scorefile 2020\n",
    "add_argument_ids_to_scorefile(ranking_file_2020, scores_file_2020, output_file_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-rank 2021 paths\n",
    "argument2021_scorefile_with_doc_ids = data_path + 'Data/scorefiles_2020_2021/sentiment_sarcasm_scorefiles/argument2021_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "merged_data_no_relevance_2021 = data_path + 'Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_no_relevance_2021.svmrank'\n",
    "re_ranked_2021 = data_path + 'Data/re_ranked_2020_2021/sentiment_sarcasm_re_ranked/re_ranked_2021_LambdaMart.txt'\n",
    "\n",
    "#Re-rank 2020 paths\n",
    "argument2020_scorefile_with_doc_ids = data_path + 'Data/scorefiles_2020_2021/sentiment_sarcasm_scorefiles/argument2020_scorefile_with_doc_ids_LambdaMart.txt'\n",
    "merged_data_no_relevance_2020 = data_path + 'Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_no_relevance_2020.svmrank'\n",
    "re_ranked_2020 = data_path + 'Data/re_ranked_2020_2021/sentiment_sarcasm_re_ranked/re_ranked_2020_LambdaMart.txt'\n",
    "\n",
    "\n",
    "# Re-rank 2020\n",
    "re_rank_svmrank_data(argument2020_scorefile_with_doc_ids, merged_data_no_relevance_2020, re_ranked_2020)\n",
    "\n",
    "#Re-rank 2021\n",
    "re_rank_svmrank_data(argument2021_scorefile_with_doc_ids, merged_data_no_relevance_2021, re_ranked_2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Extract All Features and Create SVMrank-Formatted Vectors\n",
    "\n",
    "This function extracts features from CSV files in a given directory, concatenates them into a single dataframe, converts them to SVMrank-style feature representation, and saves them as a file without headers. It takes two arguments:\n",
    "\n",
    "- `input_dir_path`: the directory path containing the CSV files.\n",
    "- `output_svmrank_path`: the path to the output file where the SVMrank-formatted feature vectors will be saved.\n",
    "\n",
    "The function reads in each CSV file in the directory, drops any rows with missing relevance scores, and extracts feature values for each file. The cleaned dataframes and feature values are stored in separate lists. The function then concatenates all dataframes into a single merged dataframe, dropping any remaining rows with NaN values. New columns are added to the merged dataframe for each feature value extracted from each file. The columns are then converted to SVMrank-style feature representation, and the feature vectors are saved as a file without headers.\n",
    "\n",
    "### Output\n",
    "The function saves the SVMrank-formatted feature vectors as a file without headers at the specified output path.\n",
    "\n",
    "### Required Libraries\n",
    "The function requires the following Python libraries:\n",
    "\n",
    "- pandas\n",
    "- os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "data_path = '/Users/balazs/Desktop/dissertationProjectCode/dissertationCodeBase/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_features_svm(input_dir_path, output_svmrank_path):\n",
    "    dataframes = []\n",
    "    feature_values = []\n",
    "\n",
    "    for filename in os.listdir(input_dir_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(input_dir_path, filename), sep=\",\", header=0)\n",
    "\n",
    "            file_feature_values = {}\n",
    "            for j, feature_name in enumerate(df.columns.drop([\"relevance_score\", \"qid\", \"docno\"]), start=1):\n",
    "                file_feature_values[feature_name] = df[feature_name].tolist()\n",
    "\n",
    "            dataframes.append(df)\n",
    "            feature_values.append(file_feature_values)\n",
    "\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "    for feature_name in feature_values[0]:\n",
    "        feature_values_flat = [val for file_dict in feature_values for val in file_dict.get(feature_name, [])]\n",
    "        \n",
    "        # Pad missing values with 0\n",
    "        if len(feature_values_flat) < len(merged_df):\n",
    "            feature_values_flat.extend([0] * (len(merged_df) - len(feature_values_flat)))\n",
    "\n",
    "        if len(feature_values_flat) == len(merged_df):\n",
    "            merged_df[feature_name] = feature_values_flat\n",
    "        else:\n",
    "            print(f\"Skipping '{feature_name}' due to length mismatch (index: {len(merged_df)}, values: {len(feature_values_flat)})\")\n",
    "\n",
    "    merged_df.sort_values(by='qid', inplace=True)\n",
    "\n",
    "    with open(output_svmrank_path, 'w') as f:\n",
    "        for _, row in merged_df.iterrows():\n",
    "            f.write(f\"{row['relevance_score']} qid:{int(row['qid'])}\")\n",
    "\n",
    "            for j, feature_name in enumerate(merged_df.columns.drop([\"relevance_score\", \"qid\", \"docno\"]), start=1):\n",
    "                f.write(f\" {j}:{row[feature_name]}\")\n",
    "\n",
    "            f.write(f\" #{row['docno']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_features_svm_no_relevance(input_dir_path, output_svmrank_path):\n",
    "    dataframes = []\n",
    "    feature_values = []\n",
    "\n",
    "    for filename in os.listdir(input_dir_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(input_dir_path, filename), sep=\",\", header=0)\n",
    "\n",
    "            file_feature_values = {}\n",
    "            for j, feature_name in enumerate(df.columns.drop([\"relevance_score\", \"qid\", \"docno\"]), start=1):\n",
    "                file_feature_values[feature_name] = df[feature_name].tolist()\n",
    "\n",
    "            dataframes.append(df)\n",
    "            feature_values.append(file_feature_values)\n",
    "\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "    for feature_name in feature_values[0]:\n",
    "        feature_values_flat = [val for file_dict in feature_values for val in file_dict.get(feature_name, [])]\n",
    "        \n",
    "        # Pad missing values with 0\n",
    "        if len(feature_values_flat) < len(merged_df):\n",
    "            feature_values_flat.extend([0] * (len(merged_df) - len(feature_values_flat)))\n",
    "\n",
    "        if len(feature_values_flat) == len(merged_df):\n",
    "            merged_df[feature_name] = feature_values_flat\n",
    "        else:\n",
    "            print(f\"Skipping '{feature_name}' due to length mismatch (index: {len(merged_df)}, values: {len(feature_values_flat)})\")\n",
    "\n",
    "    merged_df.sort_values(by='qid', inplace=True)\n",
    "\n",
    "    with open(output_svmrank_path, 'w') as f:\n",
    "        for _, row in merged_df.iterrows():\n",
    "            f.write(\"0\")  # Default target value\n",
    "            f.write(f\" qid:{int(row['qid'])}\")  # Query ID\n",
    "\n",
    "            for j, feature_name in enumerate(merged_df.columns.drop([\"relevance_score\", \"qid\", \"docno\"]), start=1):\n",
    "                f.write(f\" {j}:{row[feature_name]}\")\n",
    "\n",
    "            f.write(f\" #{row['docno']}\\n\")  # Document ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_path = data_path + 'Data/merged_features_relevance_2021/sbert_features_relevance'\n",
    "output_svmrank_path = data_path + 'Data/svm_format_2020_2021/sbert_merged/merged_data_2021.svmrank'\n",
    "output_svmrank_path_no_relevance = data_path + 'Data/svm_format_2020_2021/sbert_merged/merged_data_no_relevance_2021.svmrank'\n",
    "\n",
    "all_features_svm(input_dir_path, output_svmrank_path)\n",
    "all_features_svm_no_relevance(input_dir_path,output_svmrank_path_no_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_path = data_path + 'Data/merged_features_relevance_2021/sentiment_sarcasm_features_relevance'\n",
    "output_svmrank_path = data_path + 'Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2021.svmrank'\n",
    "output_svmrank_path_no_relevance = data_path + 'Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_no_relevance_2021.svmrank'\n",
    "\n",
    "all_features_svm(input_dir_path, output_svmrank_path)\n",
    "all_features_svm_no_relevance(input_dir_path,output_svmrank_path_no_relevance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Doesnt work, bug needs to be fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "##input_dir_path = data_path + 'Data/merged_features_relevance_2021/tf_idf_features_relevance'\n",
    "##output_svmrank_path = data_path + 'Data/svm_format_2020_2021/tf_idf_merged/merged_data_2021.svmrank'\n",
    "##output_svmrank_path_no_relevance = data_path + 'Data/svm_format_2020_2021/tf_idf_merged/merged_data_no_relevance_2021.svmrank'\n",
    "\n",
    "##all_features_svm(input_dir_path, output_svmrank_path)\n",
    "##all_features_svm_no_relevance(input_dir_path,output_svmrank_path_no_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_path = data_path + 'Data/merged_features_relevance_2020/sbert_features_relevance'\n",
    "output_svmrank_path = data_path + 'Data/svm_format_2020_2021/sbert_merged/merged_data_2020.svmrank'\n",
    "output_svmrank_path_no_relevance = data_path + 'Data/svm_format_2020_2021/sbert_merged/merged_data_no_relevance_2020.svmrank'\n",
    "\n",
    "all_features_svm(input_dir_path, output_svmrank_path)\n",
    "all_features_svm_no_relevance(input_dir_path,output_svmrank_path_no_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_path = data_path + 'Data/merged_features_relevance_2020/sentiment_sarcasm_features_relevance'\n",
    "output_svmrank_path = data_path + 'Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_2020.svmrank'\n",
    "output_svmrank_path_no_relevance = data_path + 'Data/svm_format_2020_2021/sentiment_sarcasm_merged/merged_data_no_relevance_2020.svmrank'\n",
    "\n",
    "all_features_svm(input_dir_path, output_svmrank_path)\n",
    "all_features_svm_no_relevance(input_dir_path,output_svmrank_path_no_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_path = data_path + 'Data/merged_features_relevance_2020/tf_idf_features_relevance'\n",
    "output_svmrank_path = data_path + 'Data/svm_format_2020_2021/tf_idf_merged/merged_data_2020.svmrank'\n",
    "output_svmrank_path_no_relevance = data_path + 'Data/svm_format_2020_2021/tf_idf_merged/merged_data_no_relevance_2020.svmrank'\n",
    "\n",
    "all_features_svm(input_dir_path, output_svmrank_path)\n",
    "all_features_svm_no_relevance(input_dir_path,output_svmrank_path_no_relevance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
